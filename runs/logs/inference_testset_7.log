[2025-02-20 11:03:18,722] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-02-20 11:03:57.632943: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-02-20 11:03:57.647827: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1740049437.663574  115459 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1740049437.668811  115459 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-20 11:03:57.687307: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2025-02-20 11:05:15,537] [WARNING] No training data specified
[2025-02-20 11:05:15,537] [WARNING] No train_iters (recommended) or epochs specified, use default 10k iters.
[2025-02-20 11:05:15,537] [INFO] using world size: 1
[2025-02-20 11:05:15,540] [INFO] [RANK 0] > initializing model parallel with size 1
[2025-02-20 11:05:15,628] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:23<00:23, 23.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.73s/it]
[2025-02-20 11:07:20,210] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 19524432675
[2025-02-20 11:07:27,564] [INFO] [RANK 0] global rank 0 is loading checkpoint ./pretrained_models/hallo3/1/mp_rank_00_model_states.pt
[2025-02-20 11:08:07,991] [INFO] [RANK 0] Warning: Missing keys for inference: ['conditioner.embedders.0.transformer.shared.weight', 'conditioner.embedders.0.transformer.encoder.embed_tokens.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.final_layer_norm.weight', 'first_stage_model.encoder.conv_in.conv.weight', 'first_stage_model.encoder.conv_in.conv.bias', 'first_stage_model.encoder.down.0.block.0.norm1.weight', 'first_stage_model.encoder.down.0.block.0.norm1.bias', 'first_stage_model.encoder.down.0.block.0.conv1.conv.weight', 'first_stage_model.encoder.down.0.block.0.conv1.conv.bias', 'first_stage_model.encoder.down.0.block.0.norm2.weight', 'first_stage_model.encoder.down.0.block.0.norm2.bias', 'first_stage_model.encoder.down.0.block.0.conv2.conv.weight', 'first_stage_model.encoder.down.0.block.0.conv2.conv.bias', 'first_stage_model.encoder.down.0.block.1.norm1.weight', 'first_stage_model.encoder.down.0.block.1.norm1.bias', 'first_stage_model.encoder.down.0.block.1.conv1.conv.weight', 'first_stage_model.encoder.down.0.block.1.conv1.conv.bias', 'first_stage_model.encoder.down.0.block.1.norm2.weight', 'first_stage_model.encoder.down.0.block.1.norm2.bias', 'first_stage_model.encoder.down.0.block.1.conv2.conv.weight', 'first_stage_model.encoder.down.0.block.1.conv2.conv.bias', 'first_stage_model.encoder.down.0.block.2.norm1.weight', 'first_stage_model.encoder.down.0.block.2.norm1.bias', 'first_stage_model.encoder.down.0.block.2.conv1.conv.weight', 'first_stage_model.encoder.down.0.block.2.conv1.conv.bias', 'first_stage_model.encoder.down.0.block.2.norm2.weight', 'first_stage_model.encoder.down.0.block.2.norm2.bias', 'first_stage_model.encoder.down.0.block.2.conv2.conv.weight', 'first_stage_model.encoder.down.0.block.2.conv2.conv.bias', 'first_stage_model.encoder.down.0.downsample.conv.weight', 'first_stage_model.encoder.down.0.downsample.conv.bias', 'first_stage_model.encoder.down.1.block.0.norm1.weight', 'first_stage_model.encoder.down.1.block.0.norm1.bias', 'first_stage_model.encoder.down.1.block.0.conv1.conv.weight', 'first_stage_model.encoder.down.1.block.0.conv1.conv.bias', 'first_stage_model.encoder.down.1.block.0.norm2.weight', 'first_stage_model.encoder.down.1.block.0.norm2.bias', 'first_stage_model.encoder.down.1.block.0.conv2.conv.weight', 'first_stage_model.encoder.down.1.block.0.conv2.conv.bias', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.1.block.1.norm1.weight', 'first_stage_model.encoder.down.1.block.1.norm1.bias', 'first_stage_model.encoder.down.1.block.1.conv1.conv.weight', 'first_stage_model.encoder.down.1.block.1.conv1.conv.bias', 'first_stage_model.encoder.down.1.block.1.norm2.weight', 'first_stage_model.encoder.down.1.block.1.norm2.bias', 'first_stage_model.encoder.down.1.block.1.conv2.conv.weight', 'first_stage_model.encoder.down.1.block.1.conv2.conv.bias', 'first_stage_model.encoder.down.1.block.2.norm1.weight', 'first_stage_model.encoder.down.1.block.2.norm1.bias', 'first_stage_model.encoder.down.1.block.2.conv1.conv.weight', 'first_stage_model.encoder.down.1.block.2.conv1.conv.bias', 'first_stage_model.encoder.down.1.block.2.norm2.weight', 'first_stage_model.encoder.down.1.block.2.norm2.bias', 'first_stage_model.encoder.down.1.block.2.conv2.conv.weight', 'first_stage_model.encoder.down.1.block.2.conv2.conv.bias', 'first_stage_model.encoder.down.1.downsample.conv.weight', 'first_stage_model.encoder.down.1.downsample.conv.bias', 'first_stage_model.encoder.down.2.block.0.norm1.weight', 'first_stage_model.encoder.down.2.block.0.norm1.bias', 'first_stage_model.encoder.down.2.block.0.conv1.conv.weight', 'first_stage_model.encoder.down.2.block.0.conv1.conv.bias', 'first_stage_model.encoder.down.2.block.0.norm2.weight', 'first_stage_model.encoder.down.2.block.0.norm2.bias', 'first_stage_model.encoder.down.2.block.0.conv2.conv.weight', 'first_stage_model.encoder.down.2.block.0.conv2.conv.bias', 'first_stage_model.encoder.down.2.block.1.norm1.weight', 'first_stage_model.encoder.down.2.block.1.norm1.bias', 'first_stage_model.encoder.down.2.block.1.conv1.conv.weight', 'first_stage_model.encoder.down.2.block.1.conv1.conv.bias', 'first_stage_model.encoder.down.2.block.1.norm2.weight', 'first_stage_model.encoder.down.2.block.1.norm2.bias', 'first_stage_model.encoder.down.2.block.1.conv2.conv.weight', 'first_stage_model.encoder.down.2.block.1.conv2.conv.bias', 'first_stage_model.encoder.down.2.block.2.norm1.weight', 'first_stage_model.encoder.down.2.block.2.norm1.bias', 'first_stage_model.encoder.down.2.block.2.conv1.conv.weight', 'first_stage_model.encoder.down.2.block.2.conv1.conv.bias', 'first_stage_model.encoder.down.2.block.2.norm2.weight', 'first_stage_model.encoder.down.2.block.2.norm2.bias', 'first_stage_model.encoder.down.2.block.2.conv2.conv.weight', 'first_stage_model.encoder.down.2.block.2.conv2.conv.bias', 'first_stage_model.encoder.down.2.downsample.conv.weight', 'first_stage_model.encoder.down.2.downsample.conv.bias', 'first_stage_model.encoder.down.3.block.0.norm1.weight', 'first_stage_model.encoder.down.3.block.0.norm1.bias', 'first_stage_model.encoder.down.3.block.0.conv1.conv.weight', 'first_stage_model.encoder.down.3.block.0.conv1.conv.bias', 'first_stage_model.encoder.down.3.block.0.norm2.weight', 'first_stage_model.encoder.down.3.block.0.norm2.bias', 'first_stage_model.encoder.down.3.block.0.conv2.conv.weight', 'first_stage_model.encoder.down.3.block.0.conv2.conv.bias', 'first_stage_model.encoder.down.3.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.3.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.3.block.1.norm1.weight', 'first_stage_model.encoder.down.3.block.1.norm1.bias', 'first_stage_model.encoder.down.3.block.1.conv1.conv.weight', 'first_stage_model.encoder.down.3.block.1.conv1.conv.bias', 'first_stage_model.encoder.down.3.block.1.norm2.weight', 'first_stage_model.encoder.down.3.block.1.norm2.bias', 'first_stage_model.encoder.down.3.block.1.conv2.conv.weight', 'first_stage_model.encoder.down.3.block.1.conv2.conv.bias', 'first_stage_model.encoder.down.3.block.2.norm1.weight', 'first_stage_model.encoder.down.3.block.2.norm1.bias', 'first_stage_model.encoder.down.3.block.2.conv1.conv.weight', 'first_stage_model.encoder.down.3.block.2.conv1.conv.bias', 'first_stage_model.encoder.down.3.block.2.norm2.weight', 'first_stage_model.encoder.down.3.block.2.norm2.bias', 'first_stage_model.encoder.down.3.block.2.conv2.conv.weight', 'first_stage_model.encoder.down.3.block.2.conv2.conv.bias', 'first_stage_model.encoder.mid.block_1.norm1.weight', 'first_stage_model.encoder.mid.block_1.norm1.bias', 'first_stage_model.encoder.mid.block_1.conv1.conv.weight', 'first_stage_model.encoder.mid.block_1.conv1.conv.bias', 'first_stage_model.encoder.mid.block_1.norm2.weight', 'first_stage_model.encoder.mid.block_1.norm2.bias', 'first_stage_model.encoder.mid.block_1.conv2.conv.weight', 'first_stage_model.encoder.mid.block_1.conv2.conv.bias', 'first_stage_model.encoder.mid.block_2.norm1.weight', 'first_stage_model.encoder.mid.block_2.norm1.bias', 'first_stage_model.encoder.mid.block_2.conv1.conv.weight', 'first_stage_model.encoder.mid.block_2.conv1.conv.bias', 'first_stage_model.encoder.mid.block_2.norm2.weight', 'first_stage_model.encoder.mid.block_2.norm2.bias', 'first_stage_model.encoder.mid.block_2.conv2.conv.weight', 'first_stage_model.encoder.mid.block_2.conv2.conv.bias', 'first_stage_model.encoder.norm_out.weight', 'first_stage_model.encoder.norm_out.bias', 'first_stage_model.encoder.conv_out.conv.weight', 'first_stage_model.encoder.conv_out.conv.bias', 'first_stage_model.decoder.conv_in.conv.weight', 'first_stage_model.decoder.conv_in.conv.bias', 'first_stage_model.decoder.mid.block_1.norm1.norm_layer.weight', 'first_stage_model.decoder.mid.block_1.norm1.norm_layer.bias', 'first_stage_model.decoder.mid.block_1.norm1.conv_y.conv.weight', 'first_stage_model.decoder.mid.block_1.norm1.conv_y.conv.bias', 'first_stage_model.decoder.mid.block_1.norm1.conv_b.conv.weight', 'first_stage_model.decoder.mid.block_1.norm1.conv_b.conv.bias', 'first_stage_model.decoder.mid.block_1.conv1.conv.weight', 'first_stage_model.decoder.mid.block_1.conv1.conv.bias', 'first_stage_model.decoder.mid.block_1.norm2.norm_layer.weight', 'first_stage_model.decoder.mid.block_1.norm2.norm_layer.bias', 'first_stage_model.decoder.mid.block_1.norm2.conv_y.conv.weight', 'first_stage_model.decoder.mid.block_1.norm2.conv_y.conv.bias', 'first_stage_model.decoder.mid.block_1.norm2.conv_b.conv.weight', 'first_stage_model.decoder.mid.block_1.norm2.conv_b.conv.bias', 'first_stage_model.decoder.mid.block_1.conv2.conv.weight', 'first_stage_model.decoder.mid.block_1.conv2.conv.bias', 'first_stage_model.decoder.mid.block_2.norm1.norm_layer.weight', 'first_stage_model.decoder.mid.block_2.norm1.norm_layer.bias', 'first_stage_model.decoder.mid.block_2.norm1.conv_y.conv.weight', 'first_stage_model.decoder.mid.block_2.norm1.conv_y.conv.bias', 'first_stage_model.decoder.mid.block_2.norm1.conv_b.conv.weight', 'first_stage_model.decoder.mid.block_2.norm1.conv_b.conv.bias', 'first_stage_model.decoder.mid.block_2.conv1.conv.weight', 'first_stage_model.decoder.mid.block_2.conv1.conv.bias', 'first_stage_model.decoder.mid.block_2.norm2.norm_layer.weight', 'first_stage_model.decoder.mid.block_2.norm2.norm_layer.bias', 'first_stage_model.decoder.mid.block_2.norm2.conv_y.conv.weight', 'first_stage_model.decoder.mid.block_2.norm2.conv_y.conv.bias', 'first_stage_model.decoder.mid.block_2.norm2.conv_b.conv.weight', 'first_stage_model.decoder.mid.block_2.norm2.conv_b.conv.bias', 'first_stage_model.decoder.mid.block_2.conv2.conv.weight', 'first_stage_model.decoder.mid.block_2.conv2.conv.bias', 'first_stage_model.decoder.up.0.block.0.norm1.norm_layer.weight', 'first_stage_model.decoder.up.0.block.0.norm1.norm_layer.bias', 'first_stage_model.decoder.up.0.block.0.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.0.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.0.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.0.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.0.conv1.conv.weight', 'first_stage_model.decoder.up.0.block.0.conv1.conv.bias', 'first_stage_model.decoder.up.0.block.0.norm2.norm_layer.weight', 'first_stage_model.decoder.up.0.block.0.norm2.norm_layer.bias', 'first_stage_model.decoder.up.0.block.0.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.0.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.0.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.0.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.0.conv2.conv.weight', 'first_stage_model.decoder.up.0.block.0.conv2.conv.bias', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.0.block.1.norm1.norm_layer.weight', 'first_stage_model.decoder.up.0.block.1.norm1.norm_layer.bias', 'first_stage_model.decoder.up.0.block.1.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.1.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.1.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.1.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.1.conv1.conv.weight', 'first_stage_model.decoder.up.0.block.1.conv1.conv.bias', 'first_stage_model.decoder.up.0.block.1.norm2.norm_layer.weight', 'first_stage_model.decoder.up.0.block.1.norm2.norm_layer.bias', 'first_stage_model.decoder.up.0.block.1.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.1.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.1.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.1.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.1.conv2.conv.weight', 'first_stage_model.decoder.up.0.block.1.conv2.conv.bias', 'first_stage_model.decoder.up.0.block.2.norm1.norm_layer.weight', 'first_stage_model.decoder.up.0.block.2.norm1.norm_layer.bias', 'first_stage_model.decoder.up.0.block.2.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.2.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.2.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.2.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.2.conv1.conv.weight', 'first_stage_model.decoder.up.0.block.2.conv1.conv.bias', 'first_stage_model.decoder.up.0.block.2.norm2.norm_layer.weight', 'first_stage_model.decoder.up.0.block.2.norm2.norm_layer.bias', 'first_stage_model.decoder.up.0.block.2.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.2.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.2.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.2.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.2.conv2.conv.weight', 'first_stage_model.decoder.up.0.block.2.conv2.conv.bias', 'first_stage_model.decoder.up.0.block.3.norm1.norm_layer.weight', 'first_stage_model.decoder.up.0.block.3.norm1.norm_layer.bias', 'first_stage_model.decoder.up.0.block.3.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.3.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.3.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.3.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.3.conv1.conv.weight', 'first_stage_model.decoder.up.0.block.3.conv1.conv.bias', 'first_stage_model.decoder.up.0.block.3.norm2.norm_layer.weight', 'first_stage_model.decoder.up.0.block.3.norm2.norm_layer.bias', 'first_stage_model.decoder.up.0.block.3.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.3.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.3.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.3.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.3.conv2.conv.weight', 'first_stage_model.decoder.up.0.block.3.conv2.conv.bias', 'first_stage_model.decoder.up.1.block.0.norm1.norm_layer.weight', 'first_stage_model.decoder.up.1.block.0.norm1.norm_layer.bias', 'first_stage_model.decoder.up.1.block.0.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.0.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.0.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.0.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.0.conv1.conv.weight', 'first_stage_model.decoder.up.1.block.0.conv1.conv.bias', 'first_stage_model.decoder.up.1.block.0.norm2.norm_layer.weight', 'first_stage_model.decoder.up.1.block.0.norm2.norm_layer.bias', 'first_stage_model.decoder.up.1.block.0.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.0.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.0.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.0.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.0.conv2.conv.weight', 'first_stage_model.decoder.up.1.block.0.conv2.conv.bias', 'first_stage_model.decoder.up.1.block.1.norm1.norm_layer.weight', 'first_stage_model.decoder.up.1.block.1.norm1.norm_layer.bias', 'first_stage_model.decoder.up.1.block.1.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.1.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.1.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.1.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.1.conv1.conv.weight', 'first_stage_model.decoder.up.1.block.1.conv1.conv.bias', 'first_stage_model.decoder.up.1.block.1.norm2.norm_layer.weight', 'first_stage_model.decoder.up.1.block.1.norm2.norm_layer.bias', 'first_stage_model.decoder.up.1.block.1.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.1.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.1.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.1.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.1.conv2.conv.weight', 'first_stage_model.decoder.up.1.block.1.conv2.conv.bias', 'first_stage_model.decoder.up.1.block.2.norm1.norm_layer.weight', 'first_stage_model.decoder.up.1.block.2.norm1.norm_layer.bias', 'first_stage_model.decoder.up.1.block.2.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.2.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.2.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.2.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.2.conv1.conv.weight', 'first_stage_model.decoder.up.1.block.2.conv1.conv.bias', 'first_stage_model.decoder.up.1.block.2.norm2.norm_layer.weight', 'first_stage_model.decoder.up.1.block.2.norm2.norm_layer.bias', 'first_stage_model.decoder.up.1.block.2.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.2.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.2.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.2.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.2.conv2.conv.weight', 'first_stage_model.decoder.up.1.block.2.conv2.conv.bias', 'first_stage_model.decoder.up.1.block.3.norm1.norm_layer.weight', 'first_stage_model.decoder.up.1.block.3.norm1.norm_layer.bias', 'first_stage_model.decoder.up.1.block.3.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.3.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.3.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.3.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.3.conv1.conv.weight', 'first_stage_model.decoder.up.1.block.3.conv1.conv.bias', 'first_stage_model.decoder.up.1.block.3.norm2.norm_layer.weight', 'first_stage_model.decoder.up.1.block.3.norm2.norm_layer.bias', 'first_stage_model.decoder.up.1.block.3.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.3.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.3.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.3.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.3.conv2.conv.weight', 'first_stage_model.decoder.up.1.block.3.conv2.conv.bias', 'first_stage_model.decoder.up.1.upsample.conv.weight', 'first_stage_model.decoder.up.1.upsample.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm1.norm_layer.weight', 'first_stage_model.decoder.up.2.block.0.norm1.norm_layer.bias', 'first_stage_model.decoder.up.2.block.0.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.0.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.0.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.0.conv1.conv.weight', 'first_stage_model.decoder.up.2.block.0.conv1.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm2.norm_layer.weight', 'first_stage_model.decoder.up.2.block.0.norm2.norm_layer.bias', 'first_stage_model.decoder.up.2.block.0.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.0.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.0.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.0.conv2.conv.weight', 'first_stage_model.decoder.up.2.block.0.conv2.conv.bias', 'first_stage_model.decoder.up.2.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.2.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.2.block.1.norm1.norm_layer.weight', 'first_stage_model.decoder.up.2.block.1.norm1.norm_layer.bias', 'first_stage_model.decoder.up.2.block.1.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.1.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.1.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.1.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.1.conv1.conv.weight', 'first_stage_model.decoder.up.2.block.1.conv1.conv.bias', 'first_stage_model.decoder.up.2.block.1.norm2.norm_layer.weight', 'first_stage_model.decoder.up.2.block.1.norm2.norm_layer.bias', 'first_stage_model.decoder.up.2.block.1.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.1.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.1.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.1.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.1.conv2.conv.weight', 'first_stage_model.decoder.up.2.block.1.conv2.conv.bias', 'first_stage_model.decoder.up.2.block.2.norm1.norm_layer.weight', 'first_stage_model.decoder.up.2.block.2.norm1.norm_layer.bias', 'first_stage_model.decoder.up.2.block.2.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.2.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.2.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.2.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.2.conv1.conv.weight', 'first_stage_model.decoder.up.2.block.2.conv1.conv.bias', 'first_stage_model.decoder.up.2.block.2.norm2.norm_layer.weight', 'first_stage_model.decoder.up.2.block.2.norm2.norm_layer.bias', 'first_stage_model.decoder.up.2.block.2.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.2.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.2.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.2.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.2.conv2.conv.weight', 'first_stage_model.decoder.up.2.block.2.conv2.conv.bias', 'first_stage_model.decoder.up.2.block.3.norm1.norm_layer.weight', 'first_stage_model.decoder.up.2.block.3.norm1.norm_layer.bias', 'first_stage_model.decoder.up.2.block.3.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.3.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.3.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.3.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.3.conv1.conv.weight', 'first_stage_model.decoder.up.2.block.3.conv1.conv.bias', 'first_stage_model.decoder.up.2.block.3.norm2.norm_layer.weight', 'first_stage_model.decoder.up.2.block.3.norm2.norm_layer.bias', 'first_stage_model.decoder.up.2.block.3.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.3.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.3.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.3.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.3.conv2.conv.weight', 'first_stage_model.decoder.up.2.block.3.conv2.conv.bias', 'first_stage_model.decoder.up.2.upsample.conv.weight', 'first_stage_model.decoder.up.2.upsample.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm1.norm_layer.weight', 'first_stage_model.decoder.up.3.block.0.norm1.norm_layer.bias', 'first_stage_model.decoder.up.3.block.0.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.0.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.0.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.0.conv1.conv.weight', 'first_stage_model.decoder.up.3.block.0.conv1.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm2.norm_layer.weight', 'first_stage_model.decoder.up.3.block.0.norm2.norm_layer.bias', 'first_stage_model.decoder.up.3.block.0.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.0.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.0.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.0.conv2.conv.weight', 'first_stage_model.decoder.up.3.block.0.conv2.conv.bias', 'first_stage_model.decoder.up.3.block.1.norm1.norm_layer.weight', 'first_stage_model.decoder.up.3.block.1.norm1.norm_layer.bias', 'first_stage_model.decoder.up.3.block.1.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.1.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.1.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.1.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.1.conv1.conv.weight', 'first_stage_model.decoder.up.3.block.1.conv1.conv.bias', 'first_stage_model.decoder.up.3.block.1.norm2.norm_layer.weight', 'first_stage_model.decoder.up.3.block.1.norm2.norm_layer.bias', 'first_stage_model.decoder.up.3.block.1.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.1.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.1.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.1.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.1.conv2.conv.weight', 'first_stage_model.decoder.up.3.block.1.conv2.conv.bias', 'first_stage_model.decoder.up.3.block.2.norm1.norm_layer.weight', 'first_stage_model.decoder.up.3.block.2.norm1.norm_layer.bias', 'first_stage_model.decoder.up.3.block.2.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.2.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.2.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.2.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.2.conv1.conv.weight', 'first_stage_model.decoder.up.3.block.2.conv1.conv.bias', 'first_stage_model.decoder.up.3.block.2.norm2.norm_layer.weight', 'first_stage_model.decoder.up.3.block.2.norm2.norm_layer.bias', 'first_stage_model.decoder.up.3.block.2.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.2.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.2.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.2.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.2.conv2.conv.weight', 'first_stage_model.decoder.up.3.block.2.conv2.conv.bias', 'first_stage_model.decoder.up.3.block.3.norm1.norm_layer.weight', 'first_stage_model.decoder.up.3.block.3.norm1.norm_layer.bias', 'first_stage_model.decoder.up.3.block.3.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.3.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.3.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.3.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.3.conv1.conv.weight', 'first_stage_model.decoder.up.3.block.3.conv1.conv.bias', 'first_stage_model.decoder.up.3.block.3.norm2.norm_layer.weight', 'first_stage_model.decoder.up.3.block.3.norm2.norm_layer.bias', 'first_stage_model.decoder.up.3.block.3.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.3.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.3.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.3.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.3.conv2.conv.weight', 'first_stage_model.decoder.up.3.block.3.conv2.conv.bias', 'first_stage_model.decoder.up.3.upsample.conv.weight', 'first_stage_model.decoder.up.3.upsample.conv.bias', 'first_stage_model.decoder.norm_out.norm_layer.weight', 'first_stage_model.decoder.norm_out.norm_layer.bias', 'first_stage_model.decoder.norm_out.conv_y.conv.weight', 'first_stage_model.decoder.norm_out.conv_y.conv.bias', 'first_stage_model.decoder.norm_out.conv_b.conv.weight', 'first_stage_model.decoder.norm_out.conv_b.conv.bias', 'first_stage_model.decoder.conv_out.conv.weight', 'first_stage_model.decoder.conv_out.conv.bias'].
[2025-02-20 11:08:08,000] [INFO] [RANK 0] > successfully loaded ./pretrained_models/hallo3/1/mp_rank_00_model_states.pt
Some weights of Wav2VecModel were not initialized from the model checkpoint at ./pretrained_models/wav2vec/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-02-20 11:08:13,409 - INFO - separator - Separator version 0.21.2 instantiating with output_dir: .cache/audio_preprocess, output_format: WAV
2025-02-20 11:08:13,410 - INFO - separator - Operating System: Linux #1 SMP Mon Oct 19 16:18:59 UTC 2020
2025-02-20 11:08:13,410 - INFO - separator - System: Linux Node: fgtusa4kq1ga8-0 Release: 3.10.0-1160.el7.x86_64 Machine: x86_64 Proc: x86_64
2025-02-20 11:08:13,410 - INFO - separator - Python Version: 3.10.16
2025-02-20 11:08:13,410 - INFO - separator - PyTorch Version: 2.4.0+cu121
2025-02-20 11:08:15,016 - INFO - separator - FFmpeg installed: ffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers
2025-02-20 11:08:15,035 - INFO - separator - ONNX Runtime GPU package installed with version: 1.20.1
2025-02-20 11:08:15,036 - INFO - separator - ONNX Runtime CPU package installed with version: 1.20.1
2025-02-20 11:08:15,037 - INFO - separator - CUDA is available in Torch, setting Torch device to CUDA
2025-02-20 11:08:15,037 - INFO - separator - ONNXruntime has CUDAExecutionProvider available, enabling acceleration
2025-02-20 11:08:15,037 - INFO - separator - Loading model Kim_Vocal_2.onnx...
2025-02-20 11:08:16,568 - INFO - separator - Load model duration: 00:00:01
ic| device: 0
Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False
Working with z of shape (1, 16, 32, 32) = 16384 dimensions.
Deleting key loss.logvar from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.
Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.to_logits.0.weight from state_dict.
Deleting key loss.discriminator.to_logits.0.bias from state_dict.
Deleting key loss.discriminator.to_logits.3.weight from state_dict.
Deleting key loss.discriminator.to_logits.3.bias from state_dict.
Missing keys:  []
Unexpected keys:  []
Restored from ./pretrained_models/cogvideox-5b-i2v-sat/vae/3d-vae.pt
*********************rank and world_size 0 1
examples/inference/input_testset_caption_10-7.txt
Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}
find model: ./pretrained_models/face_analysis/models/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0
Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}
find model: ./pretrained_models/face_analysis/models/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0
Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}
find model: ./pretrained_models/face_analysis/models/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0
Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}
find model: ./pretrained_models/face_analysis/models/glintr100.onnx recognition ['None', 3, 112, 112] 127.5 127.5
Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}
find model: ./pretrained_models/face_analysis/models/scrfd_10g_bnkps.onnx detection [1, 3, '?', '?'] 127.5 128.0
set det-size: (640, 640)
0it [00:00, ?it/s]2025-02-20 11:08:18,135 - INFO - separator - Starting separation process for audio_file_path: /wangbenyou/shunian/workspace/talking_face/Talking-Face-Datapipe/outputs/common/2001_3000_common_24fps_121frames/videoDdwNMpLKKMM-scene7/videovideoDdwNMpLKKMM-scene7_scene2.m4a

  0%|          | 0/2 [00:00<?, ?it/s][ACould not load symbol cuFuncGetName. Error: /usr/local/cuda/compat/lib/libcuda.so.1: undefined symbol: cuFuncGetName

 50%|█████     | 1/2 [00:01<00:01,  1.83s/it][A
100%|██████████| 2/2 [00:01<00:00,  1.19it/s][A100%|██████████| 2/2 [00:01<00:00,  1.01it/s]

  0%|          | 0/2 [00:00<?, ?it/s][A
 50%|█████     | 1/2 [00:00<00:00,  8.02it/s][A100%|██████████| 2/2 [00:00<00:00,  9.81it/s]
2025-02-20 11:08:26,241 - INFO - mdx_separator - Saving Vocals stem to videovideoDdwNMpLKKMM-scene7_scene2_(Vocals)_Kim_Vocal_2.wav...
2025-02-20 11:08:27,046 - INFO - common_separator - Audio duration is 0.00 hours (5.00 seconds).
2025-02-20 11:08:27,046 - INFO - common_separator - Using pydub for writing.
2025-02-20 11:08:27,726 - INFO - common_separator - Clearing input audio file paths, sources and stems...
2025-02-20 11:08:27,726 - INFO - separator - Separation duration: 00:00:09
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1740049711.455707  115459 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:77) display != EGL_NO_DISPLAYeglGetDisplay() returned error 0x300c
W0000 00:00:1740049711.456847  115459 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
W0000 00:00:1740049711.474863  121436 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1740049711.505527  121452 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
txt [401]
[1/4]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:07<05:59,  7.19s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:12<05:06,  6.26s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:18<04:45,  5.94s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:23<04:32,  5.80s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:29<04:23,  5.72s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:35<04:15,  5.67s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:40<04:08,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:46<04:02,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:51<03:56,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:57<03:50,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:03<03:45,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:08<03:39,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:14<03:34,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:20<03:28,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:25<03:22,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:31<03:17,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:37<03:11,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:42<03:05,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:48<03:00,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:53<02:54,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:59<02:49,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:05<02:43,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:10<02:37,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:16<02:32,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:22<02:26,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:27<02:21,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:33<02:15,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:39<02:09,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:44<02:04,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:50<01:58,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:56<01:52,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [03:01<01:47,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:07<01:41,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:12<01:36,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:18<01:30,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:24<01:24,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:29<01:19,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:35<01:13,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:41<01:07,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:46<01:02,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:52<00:56,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:58<00:50,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:03<00:45,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:09<00:39,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:15<00:33,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:20<00:28,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:26<00:22,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:32<00:16,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:37<00:11,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:43<00:05,  5.66s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:43<00:05,  5.67s/it]
[2/4]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:39,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:35,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:29,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:24,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:28<04:18,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:13,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:07,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:45<04:02,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:56,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:56<03:50,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:45,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:39,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:13<03:34,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:28,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:24<03:23,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:30<03:17,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:11,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:41<03:06,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:47<03:00,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:52<02:54,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:58<02:49,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:43,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:09<02:37,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:15<02:32,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:26,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:26<02:21,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:32<02:15,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:37<02:09,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:43<02:04,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:49<01:58,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:54<01:53,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [03:00<01:47,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:06<01:41,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:11<01:36,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:17<01:30,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:23<01:24,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:28<01:19,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:34<01:13,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:40<01:07,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:45<01:02,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:51<00:56,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:57<00:50,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:02<00:45,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:08<00:39,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:13<00:33,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:19<00:28,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:25<00:22,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:30<00:16,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:36<00:11,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:42<00:05,  5.64s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:42<00:05,  5.64s/it]
[3/4]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:39,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:35,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:29,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:24,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:28<04:18,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:13,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:07,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:01,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:56,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:56<03:50,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:45,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:39,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:13<03:33,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:28,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:24<03:22,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:30<03:17,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:11,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:41<03:06,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<03:00,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:52<02:54,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:58<02:49,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:43,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:09<02:37,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:15<02:32,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:26,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:26<02:21,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:32<02:15,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:37<02:09,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:43<02:04,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:49<01:58,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:54<01:52,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [03:00<01:47,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:41,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:11<01:35,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:17<01:30,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:22<01:24,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:28<01:19,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:34<01:13,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:39<01:07,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:45<01:02,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:51<00:56,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:56<00:50,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:02<00:45,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:08<00:39,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:13<00:33,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:19<00:28,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:25<00:22,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:30<00:16,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:36<00:11,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:41<00:05,  5.63s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:41<00:05,  5.64s/it]
[4/4]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:39,  5.58s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:35,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:29,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:23,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:28<04:18,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:12,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:07,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:01,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:56,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:56<03:50,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:45,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:39,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:13<03:33,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:28,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:24<03:22,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:29<03:17,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:11,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:41<03:05,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<03:00,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:52<02:54,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:58<02:49,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:43,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:09<02:37,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:15<02:32,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:26,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:26<02:20,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:32<02:15,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:37<02:09,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:43<02:04,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:48<01:58,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:54<01:52,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [03:00<01:47,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:41,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:11<01:35,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:17<01:30,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:22<01:24,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:28<01:18,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:34<01:13,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:39<01:07,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:45<01:02,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:50<00:56,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:56<00:50,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:02<00:45,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:07<00:39,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:13<00:33,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:19<00:28,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:24<00:22,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:30<00:16,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:36<00:11,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:41<00:05,  5.63s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:41<00:05,  5.63s/it]
                  0it [19:53, ?it/s]                  0it [19:53, ?it/s]Moviepy - Building video output/output_testset_caption/videovideoDdwNMpLKKMM-scene7_scene2_first_frame-videovideoDdwNMpLKKMM-scene7_scene2-seed_26963/000000_with_audio.mp4.
MoviePy - Writing audio in 000000_with_audioTEMP_MPY_wvf_snd.mp3

chunk:   0%|          | 0/112 [00:00<?, ?it/s, now=None][A
chunk:  32%|███▏      | 36/112 [00:00<00:00, 354.76it/s, now=None][A
chunk: 100%|██████████| 112/112 [00:00<00:00, 587.02it/s, now=None][A
                                                                   [A                  0it [19:53, ?it/s]                  0it [19:53, ?it/s]MoviePy - Done.
Moviepy - Writing video output/output_testset_caption/videovideoDdwNMpLKKMM-scene7_scene2_first_frame-videovideoDdwNMpLKKMM-scene7_scene2-seed_26963/000000_with_audio.mp4


t:   0%|          | 0/127 [00:00<?, ?it/s, now=None][A
t:  24%|██▎       | 30/127 [00:00<00:00, 296.58it/s, now=None][A
t:  47%|████▋     | 60/127 [00:00<00:00, 198.84it/s, now=None][A
t:  66%|██████▌   | 84/127 [00:00<00:00, 213.25it/s, now=None][A
t:  84%|████████▍ | 107/127 [00:00<00:00, 211.43it/s, now=None][A
                                                               [A                  0it [19:54, ?it/s]                  0it [19:54, ?it/s]1it [19:54, 1194.53s/it]2025-02-20 11:28:12,655 - INFO - separator - Starting separation process for audio_file_path: /wangbenyou/shunian/workspace/talking_face/Talking-Face-Datapipe/outputs/common/0_1000_common_24fps_121frames/videozmmBH2O2j8U-scene14/videovideozmmBH2O2j8U-scene14_scene3.m4a
Moviepy - Done !
Moviepy - video ready output/output_testset_caption/videovideoDdwNMpLKKMM-scene7_scene2_first_frame-videovideoDdwNMpLKKMM-scene7_scene2-seed_26963/000000_with_audio.mp4
saving in:  output/output_testset_caption/videovideoDdwNMpLKKMM-scene7_scene2_first_frame-videovideoDdwNMpLKKMM-scene7_scene2-seed_26963

  0%|          | 0/2 [00:00<?, ?it/s][A
 50%|█████     | 1/2 [00:00<00:00,  5.40it/s][A
100%|██████████| 2/2 [00:00<00:00,  6.36it/s][A100%|██████████| 2/2 [00:00<00:00,  6.19it/s]

  0%|          | 0/2 [00:00<?, ?it/s][A
100%|██████████| 2/2 [00:00<00:00, 14.23it/s][A100%|██████████| 2/2 [00:00<00:00, 14.20it/s]
2025-02-20 11:28:13,926 - INFO - mdx_separator - Saving Vocals stem to videovideozmmBH2O2j8U-scene14_scene3_(Vocals)_Kim_Vocal_2.wav...
2025-02-20 11:28:14,089 - INFO - common_separator - Audio duration is 0.00 hours (5.00 seconds).
2025-02-20 11:28:14,089 - INFO - common_separator - Using pydub for writing.
2025-02-20 11:28:14,532 - INFO - common_separator - Clearing input audio file paths, sources and stems...
2025-02-20 11:28:14,532 - INFO - separator - Separation duration: 00:00:01
I0000 00:00:1740050894.859688  115459 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:77) display != EGL_NO_DISPLAYeglGetDisplay() returned error 0x300c
W0000 00:00:1740050894.860432  115459 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.
W0000 00:00:1740050894.872151   12951 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1740050894.902602   12964 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
1it [19:56, 1196.88s/it]
#face is invalid: 0
[rank0]: Traceback (most recent call last):
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sample_video.py", line 450, in <module>
[rank0]:     sampling_main(args, model_cls=SATVideoDiffusionEngine)
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sample_video.py", line 274, in sampling_main
[rank0]:     face_emb, face_mask_path = image_processor.preprocess(image_path, save_path, 1.2)
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sgm/utils/image_processor.py", line 94, in preprocess
[rank0]:     get_mask(source_image_path, cache_dir, face_region_ratio)
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sgm/utils/util.py", line 517, in get_mask
[rank0]:     get_lip_mask(landmarks, height, width, os.path.join(
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sgm/utils/util.py", line 425, in get_lip_mask
[rank0]:     lip_landmarks = np.take(landmarks, lip_ids, 0)
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/numpy/core/fromnumeric.py", line 192, in take
[rank0]:     return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/numpy/core/fromnumeric.py", line 59, in _wrapfunc
[rank0]:     return bound(*args, **kwds)
[rank0]: IndexError: cannot do a non-empty take from an empty axes.
[rank0]:[W220 11:28:16.750846901 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
