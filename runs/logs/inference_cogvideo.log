[2025-02-17 07:58:42,473] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-02-17 07:59:06.340835: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-02-17 07:59:06.356093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1739779146.371420   70480 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1739779146.375791   70480 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-17 07:59:06.393964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2025-02-17 08:00:15,717] [WARNING] No training data specified
[2025-02-17 08:00:15,717] [WARNING] No train_iters (recommended) or epochs specified, use default 10k iters.
[2025-02-17 08:00:15,717] [INFO] using world size: 1
[2025-02-17 08:00:15,721] [INFO] [RANK 0] > initializing model parallel with size 1
[2025-02-17 08:00:15,842] [INFO] [RANK 0] building SATVideoDiffusionEngine model ...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:34<00:00, 17.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:34<00:00, 17.36s/it]
[2025-02-17 08:02:11,730] [INFO] [RANK 0]  > number of parameters on model parallel rank 0: 19524432675
[2025-02-17 08:02:19,452] [INFO] [RANK 0] global rank 0 is loading checkpoint ./pretrained_models/cogvideox-5b-i2v-sat/transformer/1/mp_rank_00_model_states.pt
[2025-02-17 08:02:25,290] [INFO] [RANK 0] Warning: Missing keys for inference: ['model.diffusion_model.transformer.layers.0.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.0.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.0.face_attn.query.weight', 'model.diffusion_model.transformer.layers.0.face_attn.query.bias', 'model.diffusion_model.transformer.layers.0.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.0.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.0.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.0.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.0.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.0.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.0.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.0.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.0.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.0.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.0.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.0.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.1.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.1.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.1.face_attn.query.weight', 'model.diffusion_model.transformer.layers.1.face_attn.query.bias', 'model.diffusion_model.transformer.layers.1.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.1.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.1.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.1.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.1.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.1.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.1.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.1.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.1.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.1.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.1.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.1.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.2.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.2.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.2.face_attn.query.weight', 'model.diffusion_model.transformer.layers.2.face_attn.query.bias', 'model.diffusion_model.transformer.layers.2.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.2.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.2.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.2.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.2.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.2.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.2.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.2.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.2.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.2.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.2.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.2.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.3.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.3.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.3.face_attn.query.weight', 'model.diffusion_model.transformer.layers.3.face_attn.query.bias', 'model.diffusion_model.transformer.layers.3.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.3.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.3.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.3.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.3.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.3.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.3.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.3.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.3.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.3.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.3.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.3.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.4.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.4.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.4.face_attn.query.weight', 'model.diffusion_model.transformer.layers.4.face_attn.query.bias', 'model.diffusion_model.transformer.layers.4.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.4.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.4.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.4.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.4.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.4.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.4.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.4.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.4.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.4.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.4.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.4.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.5.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.5.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.5.face_attn.query.weight', 'model.diffusion_model.transformer.layers.5.face_attn.query.bias', 'model.diffusion_model.transformer.layers.5.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.5.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.5.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.5.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.5.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.5.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.5.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.5.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.5.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.5.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.5.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.5.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.6.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.6.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.6.face_attn.query.weight', 'model.diffusion_model.transformer.layers.6.face_attn.query.bias', 'model.diffusion_model.transformer.layers.6.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.6.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.6.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.6.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.6.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.6.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.6.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.6.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.6.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.6.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.6.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.6.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.7.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.7.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.7.face_attn.query.weight', 'model.diffusion_model.transformer.layers.7.face_attn.query.bias', 'model.diffusion_model.transformer.layers.7.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.7.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.7.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.7.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.7.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.7.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.7.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.7.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.7.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.7.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.7.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.7.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.8.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.8.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.8.face_attn.query.weight', 'model.diffusion_model.transformer.layers.8.face_attn.query.bias', 'model.diffusion_model.transformer.layers.8.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.8.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.8.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.8.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.8.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.8.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.8.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.8.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.8.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.8.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.8.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.8.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.9.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.9.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.9.face_attn.query.weight', 'model.diffusion_model.transformer.layers.9.face_attn.query.bias', 'model.diffusion_model.transformer.layers.9.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.9.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.9.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.9.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.9.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.9.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.9.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.9.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.9.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.9.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.9.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.9.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.10.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.10.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.10.face_attn.query.weight', 'model.diffusion_model.transformer.layers.10.face_attn.query.bias', 'model.diffusion_model.transformer.layers.10.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.10.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.10.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.10.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.10.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.10.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.10.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.10.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.10.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.10.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.10.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.10.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.11.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.11.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.11.face_attn.query.weight', 'model.diffusion_model.transformer.layers.11.face_attn.query.bias', 'model.diffusion_model.transformer.layers.11.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.11.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.11.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.11.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.11.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.11.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.11.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.11.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.11.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.11.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.11.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.11.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.12.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.12.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.12.face_attn.query.weight', 'model.diffusion_model.transformer.layers.12.face_attn.query.bias', 'model.diffusion_model.transformer.layers.12.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.12.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.12.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.12.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.12.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.12.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.12.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.12.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.12.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.12.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.12.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.12.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.13.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.13.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.13.face_attn.query.weight', 'model.diffusion_model.transformer.layers.13.face_attn.query.bias', 'model.diffusion_model.transformer.layers.13.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.13.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.13.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.13.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.13.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.13.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.13.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.13.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.13.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.13.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.13.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.13.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.14.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.14.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.14.face_attn.query.weight', 'model.diffusion_model.transformer.layers.14.face_attn.query.bias', 'model.diffusion_model.transformer.layers.14.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.14.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.14.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.14.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.14.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.14.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.14.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.14.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.14.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.14.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.14.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.14.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.15.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.15.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.15.face_attn.query.weight', 'model.diffusion_model.transformer.layers.15.face_attn.query.bias', 'model.diffusion_model.transformer.layers.15.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.15.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.15.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.15.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.15.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.15.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.15.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.15.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.15.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.15.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.15.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.15.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.16.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.16.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.16.face_attn.query.weight', 'model.diffusion_model.transformer.layers.16.face_attn.query.bias', 'model.diffusion_model.transformer.layers.16.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.16.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.16.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.16.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.16.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.16.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.16.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.16.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.16.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.16.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.16.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.16.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.17.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.17.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.17.face_attn.query.weight', 'model.diffusion_model.transformer.layers.17.face_attn.query.bias', 'model.diffusion_model.transformer.layers.17.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.17.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.17.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.17.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.17.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.17.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.17.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.17.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.17.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.17.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.17.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.17.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.18.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.18.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.18.face_attn.query.weight', 'model.diffusion_model.transformer.layers.18.face_attn.query.bias', 'model.diffusion_model.transformer.layers.18.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.18.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.18.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.18.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.18.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.18.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.18.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.18.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.18.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.18.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.18.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.18.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.19.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.19.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.19.face_attn.query.weight', 'model.diffusion_model.transformer.layers.19.face_attn.query.bias', 'model.diffusion_model.transformer.layers.19.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.19.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.19.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.19.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.19.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.19.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.19.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.19.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.19.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.19.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.19.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.19.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.20.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.20.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.20.face_attn.query.weight', 'model.diffusion_model.transformer.layers.20.face_attn.query.bias', 'model.diffusion_model.transformer.layers.20.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.20.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.20.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.20.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.20.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.20.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.20.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.20.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.20.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.20.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.20.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.20.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.21.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.21.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.21.face_attn.query.weight', 'model.diffusion_model.transformer.layers.21.face_attn.query.bias', 'model.diffusion_model.transformer.layers.21.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.21.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.21.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.21.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.21.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.21.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.21.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.21.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.21.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.21.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.21.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.21.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.22.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.22.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.22.face_attn.query.weight', 'model.diffusion_model.transformer.layers.22.face_attn.query.bias', 'model.diffusion_model.transformer.layers.22.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.22.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.22.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.22.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.22.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.22.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.22.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.22.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.22.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.22.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.22.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.22.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.23.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.23.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.23.face_attn.query.weight', 'model.diffusion_model.transformer.layers.23.face_attn.query.bias', 'model.diffusion_model.transformer.layers.23.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.23.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.23.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.23.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.23.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.23.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.23.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.23.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.23.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.23.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.23.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.23.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.24.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.24.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.24.face_attn.query.weight', 'model.diffusion_model.transformer.layers.24.face_attn.query.bias', 'model.diffusion_model.transformer.layers.24.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.24.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.24.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.24.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.24.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.24.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.24.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.24.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.24.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.24.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.24.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.24.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.25.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.25.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.25.face_attn.query.weight', 'model.diffusion_model.transformer.layers.25.face_attn.query.bias', 'model.diffusion_model.transformer.layers.25.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.25.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.25.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.25.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.25.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.25.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.25.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.25.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.25.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.25.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.25.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.25.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.26.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.26.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.26.face_attn.query.weight', 'model.diffusion_model.transformer.layers.26.face_attn.query.bias', 'model.diffusion_model.transformer.layers.26.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.26.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.26.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.26.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.26.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.26.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.26.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.26.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.26.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.26.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.26.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.26.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.27.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.27.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.27.face_attn.query.weight', 'model.diffusion_model.transformer.layers.27.face_attn.query.bias', 'model.diffusion_model.transformer.layers.27.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.27.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.27.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.27.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.27.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.27.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.27.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.27.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.27.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.27.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.27.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.27.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.28.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.28.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.28.face_attn.query.weight', 'model.diffusion_model.transformer.layers.28.face_attn.query.bias', 'model.diffusion_model.transformer.layers.28.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.28.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.28.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.28.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.28.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.28.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.28.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.28.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.28.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.28.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.28.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.28.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.29.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.29.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.29.face_attn.query.weight', 'model.diffusion_model.transformer.layers.29.face_attn.query.bias', 'model.diffusion_model.transformer.layers.29.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.29.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.29.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.29.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.29.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.29.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.29.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.29.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.29.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.29.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.29.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.29.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.30.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.30.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.30.face_attn.query.weight', 'model.diffusion_model.transformer.layers.30.face_attn.query.bias', 'model.diffusion_model.transformer.layers.30.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.30.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.30.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.30.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.30.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.30.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.30.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.30.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.30.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.30.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.30.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.30.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.31.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.31.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.31.face_attn.query.weight', 'model.diffusion_model.transformer.layers.31.face_attn.query.bias', 'model.diffusion_model.transformer.layers.31.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.31.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.31.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.31.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.31.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.31.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.31.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.31.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.31.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.31.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.31.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.31.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.32.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.32.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.32.face_attn.query.weight', 'model.diffusion_model.transformer.layers.32.face_attn.query.bias', 'model.diffusion_model.transformer.layers.32.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.32.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.32.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.32.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.32.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.32.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.32.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.32.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.32.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.32.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.32.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.32.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.33.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.33.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.33.face_attn.query.weight', 'model.diffusion_model.transformer.layers.33.face_attn.query.bias', 'model.diffusion_model.transformer.layers.33.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.33.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.33.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.33.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.33.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.33.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.33.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.33.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.33.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.33.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.33.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.33.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.34.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.34.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.34.face_attn.query.weight', 'model.diffusion_model.transformer.layers.34.face_attn.query.bias', 'model.diffusion_model.transformer.layers.34.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.34.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.34.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.34.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.34.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.34.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.34.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.34.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.34.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.34.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.34.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.34.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.35.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.35.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.35.face_attn.query.weight', 'model.diffusion_model.transformer.layers.35.face_attn.query.bias', 'model.diffusion_model.transformer.layers.35.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.35.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.35.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.35.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.35.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.35.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.35.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.35.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.35.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.35.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.35.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.35.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.36.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.36.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.36.face_attn.query.weight', 'model.diffusion_model.transformer.layers.36.face_attn.query.bias', 'model.diffusion_model.transformer.layers.36.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.36.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.36.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.36.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.36.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.36.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.36.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.36.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.36.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.36.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.36.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.36.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.37.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.37.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.37.face_attn.query.weight', 'model.diffusion_model.transformer.layers.37.face_attn.query.bias', 'model.diffusion_model.transformer.layers.37.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.37.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.37.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.37.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.37.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.37.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.37.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.37.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.37.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.37.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.37.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.37.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.38.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.38.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.38.face_attn.query.weight', 'model.diffusion_model.transformer.layers.38.face_attn.query.bias', 'model.diffusion_model.transformer.layers.38.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.38.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.38.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.38.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.38.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.38.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.38.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.38.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.38.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.38.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.38.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.38.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.39.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.39.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.39.face_attn.query.weight', 'model.diffusion_model.transformer.layers.39.face_attn.query.bias', 'model.diffusion_model.transformer.layers.39.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.39.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.39.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.39.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.39.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.39.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.39.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.39.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.39.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.39.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.39.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.39.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.40.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.40.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.40.face_attn.query.weight', 'model.diffusion_model.transformer.layers.40.face_attn.query.bias', 'model.diffusion_model.transformer.layers.40.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.40.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.40.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.40.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.40.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.40.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.40.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.40.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.40.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.40.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.40.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.40.audio_attn.dense.bias', 'model.diffusion_model.transformer.layers.41.face_input_layernorm.weight', 'model.diffusion_model.transformer.layers.41.face_input_layernorm.bias', 'model.diffusion_model.transformer.layers.41.face_attn.query.weight', 'model.diffusion_model.transformer.layers.41.face_attn.query.bias', 'model.diffusion_model.transformer.layers.41.face_attn.key_value.weight', 'model.diffusion_model.transformer.layers.41.face_attn.key_value.bias', 'model.diffusion_model.transformer.layers.41.face_attn.dense.weight', 'model.diffusion_model.transformer.layers.41.face_attn.dense.bias', 'model.diffusion_model.transformer.layers.41.audio_input_layernorm.weight', 'model.diffusion_model.transformer.layers.41.audio_input_layernorm.bias', 'model.diffusion_model.transformer.layers.41.audio_attn.query.weight', 'model.diffusion_model.transformer.layers.41.audio_attn.query.bias', 'model.diffusion_model.transformer.layers.41.audio_attn.key_value.weight', 'model.diffusion_model.transformer.layers.41.audio_attn.key_value.bias', 'model.diffusion_model.transformer.layers.41.audio_attn.dense.weight', 'model.diffusion_model.transformer.layers.41.audio_attn.dense.bias', 'model.diffusion_model.transformer.audio_proj.proj1.weight', 'model.diffusion_model.transformer.audio_proj.proj1.bias', 'model.diffusion_model.transformer.audio_proj.proj2.weight', 'model.diffusion_model.transformer.audio_proj.proj2.bias', 'model.diffusion_model.transformer.audio_proj.proj3.weight', 'model.diffusion_model.transformer.audio_proj.proj3.bias', 'model.diffusion_model.transformer.audio_proj.norm.weight', 'model.diffusion_model.transformer.audio_proj.norm.bias', 'model.diffusion_model.transformer.audio_proj.conv1.weight', 'model.diffusion_model.transformer.audio_proj.conv1.bias', 'model.diffusion_model.transformer.face_proj.proj.weight', 'model.diffusion_model.transformer.face_proj.proj.bias', 'model.diffusion_model.transformer.face_proj.norm.weight', 'model.diffusion_model.transformer.face_proj.norm.bias', 'conditioner.embedders.0.transformer.shared.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.0.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.1.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.2.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.3.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.4.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.5.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.6.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.7.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.8.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.9.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.10.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.11.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.12.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.13.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.14.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.15.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.16.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.17.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.18.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.19.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.20.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.21.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.22.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.0.SelfAttention.q.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.0.SelfAttention.k.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.0.SelfAttention.v.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.0.SelfAttention.o.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.0.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.1.DenseReluDense.wi_0.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.1.DenseReluDense.wi_1.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.1.DenseReluDense.wo.weight', 'conditioner.embedders.0.transformer.encoder.block.23.layer.1.layer_norm.weight', 'conditioner.embedders.0.transformer.encoder.final_layer_norm.weight', 'first_stage_model.encoder.conv_in.conv.weight', 'first_stage_model.encoder.conv_in.conv.bias', 'first_stage_model.encoder.down.0.block.0.norm1.weight', 'first_stage_model.encoder.down.0.block.0.norm1.bias', 'first_stage_model.encoder.down.0.block.0.conv1.conv.weight', 'first_stage_model.encoder.down.0.block.0.conv1.conv.bias', 'first_stage_model.encoder.down.0.block.0.norm2.weight', 'first_stage_model.encoder.down.0.block.0.norm2.bias', 'first_stage_model.encoder.down.0.block.0.conv2.conv.weight', 'first_stage_model.encoder.down.0.block.0.conv2.conv.bias', 'first_stage_model.encoder.down.0.block.1.norm1.weight', 'first_stage_model.encoder.down.0.block.1.norm1.bias', 'first_stage_model.encoder.down.0.block.1.conv1.conv.weight', 'first_stage_model.encoder.down.0.block.1.conv1.conv.bias', 'first_stage_model.encoder.down.0.block.1.norm2.weight', 'first_stage_model.encoder.down.0.block.1.norm2.bias', 'first_stage_model.encoder.down.0.block.1.conv2.conv.weight', 'first_stage_model.encoder.down.0.block.1.conv2.conv.bias', 'first_stage_model.encoder.down.0.block.2.norm1.weight', 'first_stage_model.encoder.down.0.block.2.norm1.bias', 'first_stage_model.encoder.down.0.block.2.conv1.conv.weight', 'first_stage_model.encoder.down.0.block.2.conv1.conv.bias', 'first_stage_model.encoder.down.0.block.2.norm2.weight', 'first_stage_model.encoder.down.0.block.2.norm2.bias', 'first_stage_model.encoder.down.0.block.2.conv2.conv.weight', 'first_stage_model.encoder.down.0.block.2.conv2.conv.bias', 'first_stage_model.encoder.down.0.downsample.conv.weight', 'first_stage_model.encoder.down.0.downsample.conv.bias', 'first_stage_model.encoder.down.1.block.0.norm1.weight', 'first_stage_model.encoder.down.1.block.0.norm1.bias', 'first_stage_model.encoder.down.1.block.0.conv1.conv.weight', 'first_stage_model.encoder.down.1.block.0.conv1.conv.bias', 'first_stage_model.encoder.down.1.block.0.norm2.weight', 'first_stage_model.encoder.down.1.block.0.norm2.bias', 'first_stage_model.encoder.down.1.block.0.conv2.conv.weight', 'first_stage_model.encoder.down.1.block.0.conv2.conv.bias', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.1.block.1.norm1.weight', 'first_stage_model.encoder.down.1.block.1.norm1.bias', 'first_stage_model.encoder.down.1.block.1.conv1.conv.weight', 'first_stage_model.encoder.down.1.block.1.conv1.conv.bias', 'first_stage_model.encoder.down.1.block.1.norm2.weight', 'first_stage_model.encoder.down.1.block.1.norm2.bias', 'first_stage_model.encoder.down.1.block.1.conv2.conv.weight', 'first_stage_model.encoder.down.1.block.1.conv2.conv.bias', 'first_stage_model.encoder.down.1.block.2.norm1.weight', 'first_stage_model.encoder.down.1.block.2.norm1.bias', 'first_stage_model.encoder.down.1.block.2.conv1.conv.weight', 'first_stage_model.encoder.down.1.block.2.conv1.conv.bias', 'first_stage_model.encoder.down.1.block.2.norm2.weight', 'first_stage_model.encoder.down.1.block.2.norm2.bias', 'first_stage_model.encoder.down.1.block.2.conv2.conv.weight', 'first_stage_model.encoder.down.1.block.2.conv2.conv.bias', 'first_stage_model.encoder.down.1.downsample.conv.weight', 'first_stage_model.encoder.down.1.downsample.conv.bias', 'first_stage_model.encoder.down.2.block.0.norm1.weight', 'first_stage_model.encoder.down.2.block.0.norm1.bias', 'first_stage_model.encoder.down.2.block.0.conv1.conv.weight', 'first_stage_model.encoder.down.2.block.0.conv1.conv.bias', 'first_stage_model.encoder.down.2.block.0.norm2.weight', 'first_stage_model.encoder.down.2.block.0.norm2.bias', 'first_stage_model.encoder.down.2.block.0.conv2.conv.weight', 'first_stage_model.encoder.down.2.block.0.conv2.conv.bias', 'first_stage_model.encoder.down.2.block.1.norm1.weight', 'first_stage_model.encoder.down.2.block.1.norm1.bias', 'first_stage_model.encoder.down.2.block.1.conv1.conv.weight', 'first_stage_model.encoder.down.2.block.1.conv1.conv.bias', 'first_stage_model.encoder.down.2.block.1.norm2.weight', 'first_stage_model.encoder.down.2.block.1.norm2.bias', 'first_stage_model.encoder.down.2.block.1.conv2.conv.weight', 'first_stage_model.encoder.down.2.block.1.conv2.conv.bias', 'first_stage_model.encoder.down.2.block.2.norm1.weight', 'first_stage_model.encoder.down.2.block.2.norm1.bias', 'first_stage_model.encoder.down.2.block.2.conv1.conv.weight', 'first_stage_model.encoder.down.2.block.2.conv1.conv.bias', 'first_stage_model.encoder.down.2.block.2.norm2.weight', 'first_stage_model.encoder.down.2.block.2.norm2.bias', 'first_stage_model.encoder.down.2.block.2.conv2.conv.weight', 'first_stage_model.encoder.down.2.block.2.conv2.conv.bias', 'first_stage_model.encoder.down.2.downsample.conv.weight', 'first_stage_model.encoder.down.2.downsample.conv.bias', 'first_stage_model.encoder.down.3.block.0.norm1.weight', 'first_stage_model.encoder.down.3.block.0.norm1.bias', 'first_stage_model.encoder.down.3.block.0.conv1.conv.weight', 'first_stage_model.encoder.down.3.block.0.conv1.conv.bias', 'first_stage_model.encoder.down.3.block.0.norm2.weight', 'first_stage_model.encoder.down.3.block.0.norm2.bias', 'first_stage_model.encoder.down.3.block.0.conv2.conv.weight', 'first_stage_model.encoder.down.3.block.0.conv2.conv.bias', 'first_stage_model.encoder.down.3.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.3.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.3.block.1.norm1.weight', 'first_stage_model.encoder.down.3.block.1.norm1.bias', 'first_stage_model.encoder.down.3.block.1.conv1.conv.weight', 'first_stage_model.encoder.down.3.block.1.conv1.conv.bias', 'first_stage_model.encoder.down.3.block.1.norm2.weight', 'first_stage_model.encoder.down.3.block.1.norm2.bias', 'first_stage_model.encoder.down.3.block.1.conv2.conv.weight', 'first_stage_model.encoder.down.3.block.1.conv2.conv.bias', 'first_stage_model.encoder.down.3.block.2.norm1.weight', 'first_stage_model.encoder.down.3.block.2.norm1.bias', 'first_stage_model.encoder.down.3.block.2.conv1.conv.weight', 'first_stage_model.encoder.down.3.block.2.conv1.conv.bias', 'first_stage_model.encoder.down.3.block.2.norm2.weight', 'first_stage_model.encoder.down.3.block.2.norm2.bias', 'first_stage_model.encoder.down.3.block.2.conv2.conv.weight', 'first_stage_model.encoder.down.3.block.2.conv2.conv.bias', 'first_stage_model.encoder.mid.block_1.norm1.weight', 'first_stage_model.encoder.mid.block_1.norm1.bias', 'first_stage_model.encoder.mid.block_1.conv1.conv.weight', 'first_stage_model.encoder.mid.block_1.conv1.conv.bias', 'first_stage_model.encoder.mid.block_1.norm2.weight', 'first_stage_model.encoder.mid.block_1.norm2.bias', 'first_stage_model.encoder.mid.block_1.conv2.conv.weight', 'first_stage_model.encoder.mid.block_1.conv2.conv.bias', 'first_stage_model.encoder.mid.block_2.norm1.weight', 'first_stage_model.encoder.mid.block_2.norm1.bias', 'first_stage_model.encoder.mid.block_2.conv1.conv.weight', 'first_stage_model.encoder.mid.block_2.conv1.conv.bias', 'first_stage_model.encoder.mid.block_2.norm2.weight', 'first_stage_model.encoder.mid.block_2.norm2.bias', 'first_stage_model.encoder.mid.block_2.conv2.conv.weight', 'first_stage_model.encoder.mid.block_2.conv2.conv.bias', 'first_stage_model.encoder.norm_out.weight', 'first_stage_model.encoder.norm_out.bias', 'first_stage_model.encoder.conv_out.conv.weight', 'first_stage_model.encoder.conv_out.conv.bias', 'first_stage_model.decoder.conv_in.conv.weight', 'first_stage_model.decoder.conv_in.conv.bias', 'first_stage_model.decoder.mid.block_1.norm1.norm_layer.weight', 'first_stage_model.decoder.mid.block_1.norm1.norm_layer.bias', 'first_stage_model.decoder.mid.block_1.norm1.conv_y.conv.weight', 'first_stage_model.decoder.mid.block_1.norm1.conv_y.conv.bias', 'first_stage_model.decoder.mid.block_1.norm1.conv_b.conv.weight', 'first_stage_model.decoder.mid.block_1.norm1.conv_b.conv.bias', 'first_stage_model.decoder.mid.block_1.conv1.conv.weight', 'first_stage_model.decoder.mid.block_1.conv1.conv.bias', 'first_stage_model.decoder.mid.block_1.norm2.norm_layer.weight', 'first_stage_model.decoder.mid.block_1.norm2.norm_layer.bias', 'first_stage_model.decoder.mid.block_1.norm2.conv_y.conv.weight', 'first_stage_model.decoder.mid.block_1.norm2.conv_y.conv.bias', 'first_stage_model.decoder.mid.block_1.norm2.conv_b.conv.weight', 'first_stage_model.decoder.mid.block_1.norm2.conv_b.conv.bias', 'first_stage_model.decoder.mid.block_1.conv2.conv.weight', 'first_stage_model.decoder.mid.block_1.conv2.conv.bias', 'first_stage_model.decoder.mid.block_2.norm1.norm_layer.weight', 'first_stage_model.decoder.mid.block_2.norm1.norm_layer.bias', 'first_stage_model.decoder.mid.block_2.norm1.conv_y.conv.weight', 'first_stage_model.decoder.mid.block_2.norm1.conv_y.conv.bias', 'first_stage_model.decoder.mid.block_2.norm1.conv_b.conv.weight', 'first_stage_model.decoder.mid.block_2.norm1.conv_b.conv.bias', 'first_stage_model.decoder.mid.block_2.conv1.conv.weight', 'first_stage_model.decoder.mid.block_2.conv1.conv.bias', 'first_stage_model.decoder.mid.block_2.norm2.norm_layer.weight', 'first_stage_model.decoder.mid.block_2.norm2.norm_layer.bias', 'first_stage_model.decoder.mid.block_2.norm2.conv_y.conv.weight', 'first_stage_model.decoder.mid.block_2.norm2.conv_y.conv.bias', 'first_stage_model.decoder.mid.block_2.norm2.conv_b.conv.weight', 'first_stage_model.decoder.mid.block_2.norm2.conv_b.conv.bias', 'first_stage_model.decoder.mid.block_2.conv2.conv.weight', 'first_stage_model.decoder.mid.block_2.conv2.conv.bias', 'first_stage_model.decoder.up.0.block.0.norm1.norm_layer.weight', 'first_stage_model.decoder.up.0.block.0.norm1.norm_layer.bias', 'first_stage_model.decoder.up.0.block.0.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.0.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.0.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.0.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.0.conv1.conv.weight', 'first_stage_model.decoder.up.0.block.0.conv1.conv.bias', 'first_stage_model.decoder.up.0.block.0.norm2.norm_layer.weight', 'first_stage_model.decoder.up.0.block.0.norm2.norm_layer.bias', 'first_stage_model.decoder.up.0.block.0.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.0.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.0.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.0.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.0.conv2.conv.weight', 'first_stage_model.decoder.up.0.block.0.conv2.conv.bias', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.0.block.1.norm1.norm_layer.weight', 'first_stage_model.decoder.up.0.block.1.norm1.norm_layer.bias', 'first_stage_model.decoder.up.0.block.1.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.1.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.1.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.1.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.1.conv1.conv.weight', 'first_stage_model.decoder.up.0.block.1.conv1.conv.bias', 'first_stage_model.decoder.up.0.block.1.norm2.norm_layer.weight', 'first_stage_model.decoder.up.0.block.1.norm2.norm_layer.bias', 'first_stage_model.decoder.up.0.block.1.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.1.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.1.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.1.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.1.conv2.conv.weight', 'first_stage_model.decoder.up.0.block.1.conv2.conv.bias', 'first_stage_model.decoder.up.0.block.2.norm1.norm_layer.weight', 'first_stage_model.decoder.up.0.block.2.norm1.norm_layer.bias', 'first_stage_model.decoder.up.0.block.2.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.2.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.2.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.2.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.2.conv1.conv.weight', 'first_stage_model.decoder.up.0.block.2.conv1.conv.bias', 'first_stage_model.decoder.up.0.block.2.norm2.norm_layer.weight', 'first_stage_model.decoder.up.0.block.2.norm2.norm_layer.bias', 'first_stage_model.decoder.up.0.block.2.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.2.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.2.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.2.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.2.conv2.conv.weight', 'first_stage_model.decoder.up.0.block.2.conv2.conv.bias', 'first_stage_model.decoder.up.0.block.3.norm1.norm_layer.weight', 'first_stage_model.decoder.up.0.block.3.norm1.norm_layer.bias', 'first_stage_model.decoder.up.0.block.3.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.3.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.3.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.3.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.3.conv1.conv.weight', 'first_stage_model.decoder.up.0.block.3.conv1.conv.bias', 'first_stage_model.decoder.up.0.block.3.norm2.norm_layer.weight', 'first_stage_model.decoder.up.0.block.3.norm2.norm_layer.bias', 'first_stage_model.decoder.up.0.block.3.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.0.block.3.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.0.block.3.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.0.block.3.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.0.block.3.conv2.conv.weight', 'first_stage_model.decoder.up.0.block.3.conv2.conv.bias', 'first_stage_model.decoder.up.1.block.0.norm1.norm_layer.weight', 'first_stage_model.decoder.up.1.block.0.norm1.norm_layer.bias', 'first_stage_model.decoder.up.1.block.0.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.0.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.0.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.0.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.0.conv1.conv.weight', 'first_stage_model.decoder.up.1.block.0.conv1.conv.bias', 'first_stage_model.decoder.up.1.block.0.norm2.norm_layer.weight', 'first_stage_model.decoder.up.1.block.0.norm2.norm_layer.bias', 'first_stage_model.decoder.up.1.block.0.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.0.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.0.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.0.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.0.conv2.conv.weight', 'first_stage_model.decoder.up.1.block.0.conv2.conv.bias', 'first_stage_model.decoder.up.1.block.1.norm1.norm_layer.weight', 'first_stage_model.decoder.up.1.block.1.norm1.norm_layer.bias', 'first_stage_model.decoder.up.1.block.1.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.1.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.1.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.1.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.1.conv1.conv.weight', 'first_stage_model.decoder.up.1.block.1.conv1.conv.bias', 'first_stage_model.decoder.up.1.block.1.norm2.norm_layer.weight', 'first_stage_model.decoder.up.1.block.1.norm2.norm_layer.bias', 'first_stage_model.decoder.up.1.block.1.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.1.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.1.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.1.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.1.conv2.conv.weight', 'first_stage_model.decoder.up.1.block.1.conv2.conv.bias', 'first_stage_model.decoder.up.1.block.2.norm1.norm_layer.weight', 'first_stage_model.decoder.up.1.block.2.norm1.norm_layer.bias', 'first_stage_model.decoder.up.1.block.2.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.2.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.2.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.2.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.2.conv1.conv.weight', 'first_stage_model.decoder.up.1.block.2.conv1.conv.bias', 'first_stage_model.decoder.up.1.block.2.norm2.norm_layer.weight', 'first_stage_model.decoder.up.1.block.2.norm2.norm_layer.bias', 'first_stage_model.decoder.up.1.block.2.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.2.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.2.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.2.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.2.conv2.conv.weight', 'first_stage_model.decoder.up.1.block.2.conv2.conv.bias', 'first_stage_model.decoder.up.1.block.3.norm1.norm_layer.weight', 'first_stage_model.decoder.up.1.block.3.norm1.norm_layer.bias', 'first_stage_model.decoder.up.1.block.3.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.3.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.3.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.3.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.3.conv1.conv.weight', 'first_stage_model.decoder.up.1.block.3.conv1.conv.bias', 'first_stage_model.decoder.up.1.block.3.norm2.norm_layer.weight', 'first_stage_model.decoder.up.1.block.3.norm2.norm_layer.bias', 'first_stage_model.decoder.up.1.block.3.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.1.block.3.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.1.block.3.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.1.block.3.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.1.block.3.conv2.conv.weight', 'first_stage_model.decoder.up.1.block.3.conv2.conv.bias', 'first_stage_model.decoder.up.1.upsample.conv.weight', 'first_stage_model.decoder.up.1.upsample.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm1.norm_layer.weight', 'first_stage_model.decoder.up.2.block.0.norm1.norm_layer.bias', 'first_stage_model.decoder.up.2.block.0.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.0.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.0.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.0.conv1.conv.weight', 'first_stage_model.decoder.up.2.block.0.conv1.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm2.norm_layer.weight', 'first_stage_model.decoder.up.2.block.0.norm2.norm_layer.bias', 'first_stage_model.decoder.up.2.block.0.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.0.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.0.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.0.conv2.conv.weight', 'first_stage_model.decoder.up.2.block.0.conv2.conv.bias', 'first_stage_model.decoder.up.2.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.2.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.2.block.1.norm1.norm_layer.weight', 'first_stage_model.decoder.up.2.block.1.norm1.norm_layer.bias', 'first_stage_model.decoder.up.2.block.1.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.1.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.1.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.1.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.1.conv1.conv.weight', 'first_stage_model.decoder.up.2.block.1.conv1.conv.bias', 'first_stage_model.decoder.up.2.block.1.norm2.norm_layer.weight', 'first_stage_model.decoder.up.2.block.1.norm2.norm_layer.bias', 'first_stage_model.decoder.up.2.block.1.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.1.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.1.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.1.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.1.conv2.conv.weight', 'first_stage_model.decoder.up.2.block.1.conv2.conv.bias', 'first_stage_model.decoder.up.2.block.2.norm1.norm_layer.weight', 'first_stage_model.decoder.up.2.block.2.norm1.norm_layer.bias', 'first_stage_model.decoder.up.2.block.2.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.2.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.2.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.2.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.2.conv1.conv.weight', 'first_stage_model.decoder.up.2.block.2.conv1.conv.bias', 'first_stage_model.decoder.up.2.block.2.norm2.norm_layer.weight', 'first_stage_model.decoder.up.2.block.2.norm2.norm_layer.bias', 'first_stage_model.decoder.up.2.block.2.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.2.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.2.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.2.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.2.conv2.conv.weight', 'first_stage_model.decoder.up.2.block.2.conv2.conv.bias', 'first_stage_model.decoder.up.2.block.3.norm1.norm_layer.weight', 'first_stage_model.decoder.up.2.block.3.norm1.norm_layer.bias', 'first_stage_model.decoder.up.2.block.3.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.3.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.3.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.3.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.3.conv1.conv.weight', 'first_stage_model.decoder.up.2.block.3.conv1.conv.bias', 'first_stage_model.decoder.up.2.block.3.norm2.norm_layer.weight', 'first_stage_model.decoder.up.2.block.3.norm2.norm_layer.bias', 'first_stage_model.decoder.up.2.block.3.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.2.block.3.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.2.block.3.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.2.block.3.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.2.block.3.conv2.conv.weight', 'first_stage_model.decoder.up.2.block.3.conv2.conv.bias', 'first_stage_model.decoder.up.2.upsample.conv.weight', 'first_stage_model.decoder.up.2.upsample.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm1.norm_layer.weight', 'first_stage_model.decoder.up.3.block.0.norm1.norm_layer.bias', 'first_stage_model.decoder.up.3.block.0.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.0.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.0.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.0.conv1.conv.weight', 'first_stage_model.decoder.up.3.block.0.conv1.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm2.norm_layer.weight', 'first_stage_model.decoder.up.3.block.0.norm2.norm_layer.bias', 'first_stage_model.decoder.up.3.block.0.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.0.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.0.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.0.conv2.conv.weight', 'first_stage_model.decoder.up.3.block.0.conv2.conv.bias', 'first_stage_model.decoder.up.3.block.1.norm1.norm_layer.weight', 'first_stage_model.decoder.up.3.block.1.norm1.norm_layer.bias', 'first_stage_model.decoder.up.3.block.1.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.1.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.1.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.1.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.1.conv1.conv.weight', 'first_stage_model.decoder.up.3.block.1.conv1.conv.bias', 'first_stage_model.decoder.up.3.block.1.norm2.norm_layer.weight', 'first_stage_model.decoder.up.3.block.1.norm2.norm_layer.bias', 'first_stage_model.decoder.up.3.block.1.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.1.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.1.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.1.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.1.conv2.conv.weight', 'first_stage_model.decoder.up.3.block.1.conv2.conv.bias', 'first_stage_model.decoder.up.3.block.2.norm1.norm_layer.weight', 'first_stage_model.decoder.up.3.block.2.norm1.norm_layer.bias', 'first_stage_model.decoder.up.3.block.2.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.2.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.2.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.2.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.2.conv1.conv.weight', 'first_stage_model.decoder.up.3.block.2.conv1.conv.bias', 'first_stage_model.decoder.up.3.block.2.norm2.norm_layer.weight', 'first_stage_model.decoder.up.3.block.2.norm2.norm_layer.bias', 'first_stage_model.decoder.up.3.block.2.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.2.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.2.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.2.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.2.conv2.conv.weight', 'first_stage_model.decoder.up.3.block.2.conv2.conv.bias', 'first_stage_model.decoder.up.3.block.3.norm1.norm_layer.weight', 'first_stage_model.decoder.up.3.block.3.norm1.norm_layer.bias', 'first_stage_model.decoder.up.3.block.3.norm1.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.3.norm1.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.3.norm1.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.3.norm1.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.3.conv1.conv.weight', 'first_stage_model.decoder.up.3.block.3.conv1.conv.bias', 'first_stage_model.decoder.up.3.block.3.norm2.norm_layer.weight', 'first_stage_model.decoder.up.3.block.3.norm2.norm_layer.bias', 'first_stage_model.decoder.up.3.block.3.norm2.conv_y.conv.weight', 'first_stage_model.decoder.up.3.block.3.norm2.conv_y.conv.bias', 'first_stage_model.decoder.up.3.block.3.norm2.conv_b.conv.weight', 'first_stage_model.decoder.up.3.block.3.norm2.conv_b.conv.bias', 'first_stage_model.decoder.up.3.block.3.conv2.conv.weight', 'first_stage_model.decoder.up.3.block.3.conv2.conv.bias', 'first_stage_model.decoder.up.3.upsample.conv.weight', 'first_stage_model.decoder.up.3.upsample.conv.bias', 'first_stage_model.decoder.norm_out.norm_layer.weight', 'first_stage_model.decoder.norm_out.norm_layer.bias', 'first_stage_model.decoder.norm_out.conv_y.conv.weight', 'first_stage_model.decoder.norm_out.conv_y.conv.bias', 'first_stage_model.decoder.norm_out.conv_b.conv.weight', 'first_stage_model.decoder.norm_out.conv_b.conv.bias', 'first_stage_model.decoder.conv_out.conv.weight', 'first_stage_model.decoder.conv_out.conv.bias', 'ref_model.diffusion_model.mixins.pos_embed.pos_embedding', 'ref_model.diffusion_model.mixins.pos_embed.freqs_sin', 'ref_model.diffusion_model.mixins.pos_embed.freqs_cos', 'ref_model.diffusion_model.mixins.patch_embed.proj.weight', 'ref_model.diffusion_model.mixins.patch_embed.proj.bias', 'ref_model.diffusion_model.mixins.patch_embed.text_proj.weight', 'ref_model.diffusion_model.mixins.patch_embed.text_proj.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.0.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.0.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.1.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.1.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.2.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.2.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.3.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.3.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.4.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.4.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.5.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.5.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.6.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.6.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.7.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.7.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.8.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.8.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.9.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.9.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.10.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.10.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.11.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.11.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.12.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.12.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.13.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.13.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.14.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.14.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.15.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.15.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.16.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.16.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.17.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.17.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.18.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.18.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.19.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.19.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.20.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.20.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.21.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.21.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.22.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.22.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.23.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.23.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.24.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.24.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.25.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.25.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.26.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.26.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.27.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.27.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.28.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.28.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.29.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.29.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.30.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.30.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.31.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.31.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.32.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.32.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.33.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.33.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.34.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.34.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.35.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.35.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.36.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.36.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.37.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.37.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.38.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.38.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.39.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.39.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.40.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.40.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.41.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.41.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.0.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.0.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.2.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.2.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.3.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.3.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.4.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.4.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.5.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.5.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.6.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.6.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.7.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.7.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.8.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.8.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.9.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.9.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.10.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.10.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.11.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.11.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.12.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.12.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.13.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.13.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.14.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.14.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.15.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.15.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.16.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.16.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.17.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.17.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.18.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.18.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.19.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.19.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.20.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.20.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.21.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.21.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.22.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.22.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.23.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.23.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.24.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.24.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.25.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.25.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.26.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.26.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.27.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.27.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.28.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.28.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.29.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.29.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.30.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.30.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.31.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.31.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.32.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.32.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.33.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.33.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.34.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.34.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.35.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.35.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.36.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.36.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.37.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.37.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.38.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.38.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.39.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.39.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.40.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.40.bias', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.41.weight', 'ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.41.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.0.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.0.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.1.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.1.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.2.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.2.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.3.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.3.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.4.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.4.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.5.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.5.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.6.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.6.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.7.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.7.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.8.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.8.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.9.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.9.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.10.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.10.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.11.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.11.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.12.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.12.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.13.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.13.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.14.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.14.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.15.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.15.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.16.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.16.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.17.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.17.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.18.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.18.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.19.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.19.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.20.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.20.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.21.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.21.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.22.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.22.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.23.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.23.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.24.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.24.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.25.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.25.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.26.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.26.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.27.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.27.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.28.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.28.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.29.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.29.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.30.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.30.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.31.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.31.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.32.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.32.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.33.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.33.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.34.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.34.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.35.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.35.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.36.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.36.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.37.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.37.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.38.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.38.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.39.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.39.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.40.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.40.bias', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.41.weight', 'ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.41.bias', 'ref_model.diffusion_model.mixins.final_layer.norm_final.weight', 'ref_model.diffusion_model.mixins.final_layer.norm_final.bias', 'ref_model.diffusion_model.mixins.final_layer.linear.weight', 'ref_model.diffusion_model.mixins.final_layer.linear.bias', 'ref_model.diffusion_model.mixins.final_layer.adaLN_modulation.1.weight', 'ref_model.diffusion_model.mixins.final_layer.adaLN_modulation.1.bias', 'ref_model.diffusion_model.transformer.position_embeddings.weight', 'ref_model.diffusion_model.transformer.layers.0.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.0.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.0.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.0.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.0.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.0.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.0.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.0.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.0.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.0.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.0.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.0.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.1.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.1.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.1.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.1.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.1.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.1.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.1.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.1.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.1.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.1.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.1.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.1.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.2.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.2.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.2.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.2.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.2.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.2.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.2.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.2.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.2.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.2.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.2.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.2.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.3.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.3.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.3.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.3.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.3.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.3.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.3.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.3.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.3.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.3.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.3.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.3.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.4.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.4.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.4.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.4.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.4.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.4.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.4.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.4.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.4.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.4.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.4.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.4.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.5.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.5.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.5.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.5.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.5.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.5.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.5.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.5.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.5.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.5.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.5.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.5.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.6.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.6.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.6.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.6.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.6.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.6.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.6.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.6.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.6.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.6.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.6.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.6.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.7.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.7.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.7.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.7.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.7.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.7.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.7.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.7.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.7.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.7.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.7.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.7.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.8.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.8.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.8.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.8.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.8.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.8.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.8.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.8.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.8.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.8.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.8.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.8.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.9.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.9.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.9.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.9.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.9.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.9.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.9.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.9.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.9.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.9.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.9.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.9.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.10.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.10.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.10.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.10.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.10.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.10.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.10.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.10.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.10.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.10.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.10.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.10.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.11.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.11.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.11.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.11.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.11.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.11.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.11.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.11.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.11.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.11.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.11.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.11.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.12.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.12.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.12.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.12.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.12.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.12.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.12.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.12.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.12.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.12.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.12.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.12.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.13.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.13.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.13.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.13.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.13.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.13.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.13.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.13.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.13.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.13.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.13.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.13.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.14.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.14.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.14.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.14.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.14.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.14.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.14.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.14.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.14.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.14.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.14.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.14.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.15.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.15.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.15.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.15.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.15.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.15.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.15.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.15.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.15.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.15.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.15.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.15.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.16.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.16.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.16.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.16.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.16.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.16.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.16.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.16.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.16.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.16.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.16.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.16.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.17.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.17.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.17.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.17.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.17.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.17.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.17.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.17.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.17.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.17.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.17.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.17.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.18.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.18.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.18.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.18.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.18.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.18.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.18.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.18.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.18.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.18.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.18.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.18.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.19.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.19.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.19.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.19.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.19.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.19.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.19.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.19.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.19.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.19.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.19.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.19.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.20.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.20.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.20.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.20.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.20.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.20.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.20.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.20.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.20.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.20.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.20.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.20.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.21.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.21.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.21.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.21.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.21.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.21.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.21.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.21.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.21.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.21.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.21.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.21.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.22.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.22.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.22.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.22.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.22.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.22.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.22.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.22.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.22.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.22.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.22.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.22.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.23.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.23.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.23.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.23.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.23.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.23.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.23.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.23.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.23.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.23.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.23.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.23.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.24.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.24.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.24.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.24.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.24.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.24.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.24.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.24.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.24.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.24.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.24.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.24.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.25.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.25.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.25.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.25.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.25.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.25.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.25.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.25.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.25.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.25.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.25.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.25.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.26.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.26.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.26.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.26.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.26.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.26.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.26.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.26.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.26.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.26.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.26.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.26.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.27.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.27.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.27.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.27.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.27.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.27.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.27.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.27.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.27.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.27.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.27.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.27.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.28.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.28.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.28.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.28.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.28.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.28.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.28.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.28.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.28.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.28.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.28.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.28.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.29.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.29.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.29.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.29.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.29.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.29.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.29.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.29.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.29.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.29.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.29.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.29.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.30.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.30.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.30.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.30.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.30.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.30.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.30.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.30.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.30.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.30.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.30.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.30.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.31.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.31.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.31.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.31.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.31.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.31.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.31.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.31.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.31.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.31.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.31.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.31.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.32.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.32.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.32.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.32.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.32.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.32.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.32.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.32.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.32.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.32.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.32.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.32.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.33.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.33.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.33.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.33.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.33.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.33.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.33.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.33.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.33.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.33.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.33.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.33.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.34.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.34.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.34.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.34.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.34.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.34.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.34.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.34.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.34.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.34.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.34.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.34.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.35.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.35.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.35.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.35.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.35.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.35.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.35.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.35.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.35.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.35.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.35.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.35.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.36.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.36.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.36.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.36.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.36.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.36.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.36.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.36.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.36.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.36.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.36.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.36.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.37.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.37.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.37.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.37.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.37.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.37.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.37.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.37.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.37.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.37.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.37.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.37.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.38.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.38.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.38.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.38.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.38.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.38.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.38.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.38.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.38.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.38.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.38.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.38.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.39.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.39.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.39.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.39.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.39.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.39.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.39.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.39.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.39.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.39.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.39.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.39.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.40.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.40.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.40.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.40.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.40.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.40.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.40.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.40.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.40.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.40.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.40.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.40.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.layers.41.input_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.41.input_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.41.attention.query_key_value.weight', 'ref_model.diffusion_model.transformer.layers.41.attention.query_key_value.bias', 'ref_model.diffusion_model.transformer.layers.41.attention.dense.weight', 'ref_model.diffusion_model.transformer.layers.41.attention.dense.bias', 'ref_model.diffusion_model.transformer.layers.41.post_attention_layernorm.weight', 'ref_model.diffusion_model.transformer.layers.41.post_attention_layernorm.bias', 'ref_model.diffusion_model.transformer.layers.41.mlp.dense_h_to_4h.weight', 'ref_model.diffusion_model.transformer.layers.41.mlp.dense_h_to_4h.bias', 'ref_model.diffusion_model.transformer.layers.41.mlp.dense_4h_to_h.weight', 'ref_model.diffusion_model.transformer.layers.41.mlp.dense_4h_to_h.bias', 'ref_model.diffusion_model.transformer.final_layernorm.weight', 'ref_model.diffusion_model.transformer.final_layernorm.bias', 'ref_model.diffusion_model.time_embed.0.weight', 'ref_model.diffusion_model.time_embed.0.bias', 'ref_model.diffusion_model.time_embed.2.weight', 'ref_model.diffusion_model.time_embed.2.bias'].
[2025-02-17 08:02:25,297] [INFO] [RANK 0] > successfully loaded ./pretrained_models/cogvideox-5b-i2v-sat/transformer/1/mp_rank_00_model_states.pt
Some weights of Wav2VecModel were not initialized from the model checkpoint at ./pretrained_models/wav2vec/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-02-17 08:02:28,176 - INFO - separator - Separator version 0.21.2 instantiating with output_dir: .cache/audio_preprocess, output_format: WAV
2025-02-17 08:02:28,176 - INFO - separator - Operating System: Linux #1 SMP Mon Oct 19 16:18:59 UTC 2020
2025-02-17 08:02:28,176 - INFO - separator - System: Linux Node: acbd10enju1kr-0 Release: 3.10.0-1160.el7.x86_64 Machine: x86_64 Proc: x86_64
2025-02-17 08:02:28,176 - INFO - separator - Python Version: 3.10.16
2025-02-17 08:02:28,176 - INFO - separator - PyTorch Version: 2.4.0+cu121
2025-02-17 08:02:29,033 - INFO - separator - FFmpeg installed: ffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers
2025-02-17 08:02:29,047 - INFO - separator - ONNX Runtime GPU package installed with version: 1.20.1
2025-02-17 08:02:29,049 - INFO - separator - ONNX Runtime CPU package installed with version: 1.20.1
2025-02-17 08:02:29,049 - INFO - separator - CUDA is available in Torch, setting Torch device to CUDA
2025-02-17 08:02:29,049 - INFO - separator - ONNXruntime has CUDAExecutionProvider available, enabling acceleration
2025-02-17 08:02:29,049 - INFO - separator - Loading model Kim_Vocal_2.onnx...
2025-02-17 08:02:30,491 - INFO - separator - Load model duration: 00:00:01
ic| device: 0
Initialized embedder #0: FrozenT5Embedder with 4762310656 params. Trainable: False
Working with z of shape (1, 16, 32, 32) = 16384 dimensions.
Deleting key loss.logvar from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.shift from state_dict.
Deleting key loss.perceptual_loss.scaling_layer.scale from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.0.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice1.2.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.5.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice2.7.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.10.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.12.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice3.14.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.17.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.19.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice4.21.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.24.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.26.bias from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.weight from state_dict.
Deleting key loss.perceptual_loss.net.slice5.28.bias from state_dict.
Deleting key loss.perceptual_loss.lin0.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin1.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin2.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin3.model.1.weight from state_dict.
Deleting key loss.perceptual_loss.lin4.model.1.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.0.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.1.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.2.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample_res.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.0.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.net.2.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.weight from state_dict.
Deleting key loss.discriminator.blocks.3.downsample.conv.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.4.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.4.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.weight from state_dict.
Deleting key loss.discriminator.blocks.5.0.downsample.1.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.5.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.conv_res.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.0.net.2.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_q.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_kv.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.0.fn.attn.to_out.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.norm.gamma from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.0.bias from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.weight from state_dict.
Deleting key loss.discriminator.blocks.6.1.1.fn.net.2.bias from state_dict.
Deleting key loss.discriminator.to_logits.0.weight from state_dict.
Deleting key loss.discriminator.to_logits.0.bias from state_dict.
Deleting key loss.discriminator.to_logits.3.weight from state_dict.
Deleting key loss.discriminator.to_logits.3.bias from state_dict.
Missing keys:  []
Unexpected keys:  []
Restored from ./pretrained_models/cogvideox-5b-i2v-sat/vae/3d-vae.pt
*********************rank and world_size 0 1
./examples/inference/input_v2.txt
Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}
find model: ./pretrained_models/face_analysis/models/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0
Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}
find model: ./pretrained_models/face_analysis/models/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0
Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}
find model: ./pretrained_models/face_analysis/models/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0
Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}
find model: ./pretrained_models/face_analysis/models/glintr100.onnx recognition ['None', 3, 112, 112] 127.5 127.5
Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}
find model: ./pretrained_models/face_analysis/models/scrfd_10g_bnkps.onnx detection [1, 3, '?', '?'] 127.5 128.0
set det-size: (640, 640)
0it [00:00, ?it/s]2025-02-17 08:02:31,737 - INFO - separator - Starting separation process for audio_file_path: examples/inference/audios/0003.wav

  0%|          | 0/3 [00:00<?, ?it/s][ACould not load symbol cuFuncGetName. Error: /usr/local/cuda/compat/lib/libcuda.so.1: undefined symbol: cuFuncGetName

 33%|███▎      | 1/3 [00:01<00:02,  1.11s/it][A
 67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s][A100%|██████████| 3/3 [00:01<00:00,  2.29it/s]

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|██████▋   | 2/3 [00:00<00:00, 16.03it/s][A100%|██████████| 3/3 [00:00<00:00, 16.30it/s]
2025-02-17 08:02:38,914 - INFO - mdx_separator - Saving Vocals stem to 0003_(Vocals)_Kim_Vocal_2.wav...
2025-02-17 08:02:39,236 - INFO - common_separator - Audio duration is 0.00 hours (9.45 seconds).
2025-02-17 08:02:39,237 - INFO - common_separator - Using pydub for writing.
2025-02-17 08:02:39,723 - INFO - common_separator - Clearing input audio file paths, sources and stems...
2025-02-17 08:02:39,723 - INFO - separator - Separation duration: 00:00:07
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739779362.893949   70480 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:77) display != EGL_NO_DISPLAYeglGetDisplay() returned error 0x300c
W0000 00:00:1739779362.937589   70480 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
W0000 00:00:1739779362.950276  106729 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1739779362.985123  106742 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
[1739779363.090441] [acbd10enju1kr-0:70480:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device
txt [73]
[1/6]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:07<06:00,  7.21s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:12<05:06,  6.26s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:18<04:45,  5.95s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:23<04:32,  5.81s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:29<04:23,  5.73s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:35<04:15,  5.68s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:40<04:08,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:46<04:02,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:51<03:56,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:57<03:50,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:03<03:44,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:08<03:38,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:14<03:33,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:20<03:27,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:25<03:21,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:31<03:16,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:36<03:10,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:42<03:05,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:48<02:59,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:53<02:54,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:59<02:48,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:04<02:42,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:10<02:37,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:16<02:31,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:21<02:26,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:27<02:20,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:33<02:14,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:38<02:09,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:44<02:03,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:49<01:58,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:55<01:52,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [03:01<01:46,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:06<01:40,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:12<01:35,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:17<01:29,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:23<01:23,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:29<01:18,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:34<01:12,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:40<01:07,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:45<01:01,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:51<00:56,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:57<00:50,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:02<00:44,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:08<00:39,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:14<00:33,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:19<00:28,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:25<00:22,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:30<00:16,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:36<00:11,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:42<00:05,  5.62s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:42<00:05,  5.64s/it]
[2/6]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:38,  5.57s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:34,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:29,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:23,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:28<04:18,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:12,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:07,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:01,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:56,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:56<03:50,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:44,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:39,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:13<03:33,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:27,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:24<03:22,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:29<03:16,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:11,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:41<03:05,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<03:00,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:52<02:54,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:58<02:48,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:43,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:09<02:37,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:14<02:31,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:26,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:26<02:20,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:31<02:15,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:37<02:09,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:43<02:03,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:48<01:58,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:54<01:52,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [02:59<01:46,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:41,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:11<01:35,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:16<01:30,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:22<01:24,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:28<01:18,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:33<01:12,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:39<01:07,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:44<01:01,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:50<00:56,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:56<00:50,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:01<00:44,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:07<00:39,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:12<00:33,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:18<00:28,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:24<00:22,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:29<00:16,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:35<00:11,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:40<00:05,  5.61s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:40<00:05,  5.62s/it]
[3/6]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:38,  5.57s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:35,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:29,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:23,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:28<04:18,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:12,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:07,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:01,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:56,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:56<03:50,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:44,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:39,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:13<03:33,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:27,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:24<03:22,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:29<03:16,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:11,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:41<03:05,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<03:00,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:52<02:54,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:58<02:48,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:43,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:09<02:37,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:14<02:32,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:26,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:26<02:20,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:31<02:15,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:37<02:09,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:43<02:03,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:48<01:58,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:54<01:52,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [03:00<01:47,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:41,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:11<01:35,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:16<01:30,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:22<01:24,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:28<01:18,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:33<01:13,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:39<01:07,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:45<01:01,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:50<00:56,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:56<00:50,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:01<00:45,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:07<00:39,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:13<00:33,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:18<00:28,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:24<00:22,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:30<00:16,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:35<00:11,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:41<00:05,  5.61s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:41<00:05,  5.63s/it]
[4/6]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:38,  5.57s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:34,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:28,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:22,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:27<04:17,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:11,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:05,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:00,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:54,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:55<03:49,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:43,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:37,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:12<03:32,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:26,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:23<03:21,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:29<03:16,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:10,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:40<03:05,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<02:59,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:51<02:54,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:57<02:48,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:42,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:08<02:37,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:14<02:31,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:26,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:25<02:20,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:31<02:15,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:36<02:09,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:42<02:03,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:48<01:58,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:53<01:52,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [02:59<01:47,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:41,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:10<01:35,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:16<01:30,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:22<01:24,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:27<01:18,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:33<01:12,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:38<01:07,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:44<01:01,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:50<00:56,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:55<00:50,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:01<00:44,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:06<00:39,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:12<00:33,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:18<00:28,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:23<00:22,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:29<00:16,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:34<00:11,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:40<00:05,  5.61s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:40<00:05,  5.61s/it]
[5/6]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:38,  5.58s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:35,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:29,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:24,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:28<04:18,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:13,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:07,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:02,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:56,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:56<03:50,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:45,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:39,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:13<03:33,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:27,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:24<03:22,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:29<03:16,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:11,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:41<03:05,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<03:00,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:52<02:54,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:58<02:48,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:43,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:09<02:37,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:14<02:32,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:26,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:26<02:20,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:31<02:15,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:37<02:09,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:43<02:03,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:48<01:58,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:54<01:52,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [03:00<01:46,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:41,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:11<01:35,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:16<01:30,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:22<01:24,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:28<01:18,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:33<01:13,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:39<01:07,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:45<01:01,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:50<00:56,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:56<00:50,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:01<00:44,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:07<00:39,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:13<00:33,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:18<00:28,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:24<00:22,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:30<00:16,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:35<00:11,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:41<00:05,  5.62s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:41<00:05,  5.63s/it]
[6/6]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:38,  5.58s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:34,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:29,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:23,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:28<04:17,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:11,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:06,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:00,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:54,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:55<03:49,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:43,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:38,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:12<03:32,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:27,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:23<03:21,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:29<03:16,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:10,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:40<03:05,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<02:59,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:52<02:54,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:57<02:48,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:43,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:08<02:37,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:14<02:31,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:26,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:25<02:20,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:31<02:15,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:37<02:09,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:42<02:03,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:48<01:58,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:53<01:52,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [02:59<01:46,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:41,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:10<01:35,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:16<01:30,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:22<01:24,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:27<01:18,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:33<01:13,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:39<01:07,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:44<01:01,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:50<00:56,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:55<00:50,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:01<00:44,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:07<00:39,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:12<00:33,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:18<00:28,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:24<00:22,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:29<00:16,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:35<00:11,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:40<00:05,  5.62s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:40<00:05,  5.62s/it]
                  0it [29:27, ?it/s]                  0it [29:27, ?it/s]Moviepy - Building video ./output_trained/0003-0003-seed_14505/000000_with_audio.mp4.
MoviePy - Writing audio in 000000_with_audioTEMP_MPY_wvf_snd.mp3

chunk:   0%|          | 0/209 [00:00<?, ?it/s, now=None][A
chunk:  95%|█████████▌| 199/209 [00:00<00:00, 1987.60it/s, now=None][A
                                                                    [A                  0it [29:27, ?it/s]                  0it [29:27, ?it/s]MoviePy - Done.
Moviepy - Writing video ./output_trained/0003-0003-seed_14505/000000_with_audio.mp4


t:   0%|          | 0/237 [00:00<?, ?it/s, now=None][A
t:  24%|██▍       | 57/237 [00:00<00:00, 363.70it/s, now=None][A
t:  40%|███▉      | 94/237 [00:00<00:00, 301.88it/s, now=None][A
t:  52%|█████▏    | 124/237 [00:00<00:00, 258.34it/s, now=None][A
t:  63%|██████▎   | 150/237 [00:00<00:00, 238.59it/s, now=None][A
t:  74%|███████▍  | 175/237 [00:00<00:00, 237.82it/s, now=None][A
t:  84%|████████▍ | 199/237 [00:00<00:00, 205.16it/s, now=None][A
t:  95%|█████████▍| 225/237 [00:00<00:00, 218.04it/s, now=None][A
                                                               [A                  0it [29:29, ?it/s]                  0it [29:29, ?it/s]1it [29:29, 1769.30s/it]2025-02-17 08:32:01,038 - INFO - separator - Starting separation process for audio_file_path: examples/inference/audios/0001.wav
Moviepy - Done !
Moviepy - video ready ./output_trained/0003-0003-seed_14505/000000_with_audio.mp4
saving in:  ./output_trained/0003-0003-seed_14505

  0%|          | 0/3 [00:00<?, ?it/s][A
 33%|███▎      | 1/3 [00:00<00:00,  5.49it/s][A
 67%|██████▋   | 2/3 [00:00<00:00,  6.97it/s][A100%|██████████| 3/3 [00:00<00:00,  7.54it/s]

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|██████▋   | 2/3 [00:00<00:00, 15.48it/s][A100%|██████████| 3/3 [00:00<00:00, 15.86it/s]
2025-02-17 08:32:01,686 - INFO - mdx_separator - Saving Vocals stem to 0001_(Vocals)_Kim_Vocal_2.wav...
2025-02-17 08:32:01,687 - INFO - common_separator - Audio duration is 0.00 hours (9.78 seconds).
2025-02-17 08:32:01,687 - INFO - common_separator - Using pydub for writing.
2025-02-17 08:32:02,698 - INFO - common_separator - Clearing input audio file paths, sources and stems...
2025-02-17 08:32:02,698 - INFO - separator - Separation duration: 00:00:01
I0000 00:00:1739781123.510144   70480 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:77) display != EGL_NO_DISPLAYeglGetDisplay() returned error 0x300c
W0000 00:00:1739781123.510702   70480 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.
W0000 00:00:1739781123.520018  121917 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1739781123.533060  121933 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
txt [39]
[1/6]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:51,  5.83s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:39,  5.70s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:17<04:30,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:24,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:28<04:17,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:11,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:06,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:00,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:55,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:56<03:49,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:44,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:38,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:12<03:33,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:27,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:24<03:22,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:29<03:16,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:11,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:41<03:05,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<02:59,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:52<02:54,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:58<02:48,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:43,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:09<02:37,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:14<02:32,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:26,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:26<02:20,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:31<02:15,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:37<02:09,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:43<02:03,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:48<01:58,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:54<01:52,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [02:59<01:47,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:41,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:11<01:35,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:16<01:29,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:22<01:24,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:28<01:18,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:33<01:12,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:39<01:07,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:44<01:01,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:50<00:55,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:55<00:50,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:01<00:44,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:07<00:39,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:12<00:33,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:18<00:28,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:24<00:22,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:29<00:16,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:35<00:11,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:40<00:05,  5.61s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:40<00:05,  5.62s/it]
[2/6]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:38,  5.57s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:35,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:29,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:24,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:28<04:18,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:12,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:07,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:01,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:56,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:56<03:50,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:45,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:39,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:13<03:33,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:28,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:24<03:22,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:30<03:17,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:11,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:41<03:05,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<03:00,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:52<02:54,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:58<02:48,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:42,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:09<02:37,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:14<02:31,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:25,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:26<02:20,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:31<02:14,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:37<02:08,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:42<02:03,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:48<01:57,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:54<01:52,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [02:59<01:46,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:41,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:11<01:35,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:16<01:30,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:22<01:24,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:28<01:18,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:33<01:13,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:39<01:07,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:44<01:01,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:50<00:56,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:56<00:50,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:01<00:45,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:07<00:39,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:13<00:33,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:18<00:28,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:24<00:22,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:29<00:16,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:35<00:11,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:41<00:05,  5.63s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:41<00:05,  5.62s/it]
[3/6]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:38,  5.57s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:35,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:29,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:23,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:28<04:17,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:11,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:06,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:00,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:54,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:55<03:49,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:43,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:38,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:12<03:32,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:27,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:23<03:21,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:29<03:16,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:10,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:40<03:05,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<02:59,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:52<02:54,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:57<02:48,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:43,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:08<02:37,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:14<02:32,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:26,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:25<02:20,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:31<02:15,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:37<02:09,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:42<02:04,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:48<01:58,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:54<01:52,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [02:59<01:47,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:41,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:10<01:35,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:16<01:29,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:22<01:24,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:27<01:18,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:33<01:12,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:38<01:07,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:44<01:01,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:50<00:56,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:55<00:50,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:01<00:44,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:07<00:39,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:12<00:33,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:18<00:28,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:23<00:22,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:29<00:16,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:35<00:11,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:40<00:05,  5.61s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:40<00:05,  5.61s/it]
[4/6]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:38,  5.57s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:34,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:29,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:23,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:28<04:18,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:12,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:07,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:01,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:56,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:56<03:50,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:44,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:39,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:13<03:33,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:28,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:24<03:22,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:29<03:17,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:11,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:41<03:05,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<03:00,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:52<02:54,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:58<02:48,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:42,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:09<02:37,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:14<02:31,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:25,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:26<02:20,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:31<02:14,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:37<02:08,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:42<02:03,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:48<01:57,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:54<01:52,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [02:59<01:46,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:41,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:11<01:35,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:16<01:30,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:22<01:24,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:27<01:18,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:33<01:13,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:39<01:07,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:44<01:01,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:50<00:56,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:56<00:50,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:01<00:45,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:07<00:39,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:12<00:33,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:18<00:28,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:24<00:22,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:29<00:16,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:35<00:11,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:41<00:05,  5.62s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:41<00:05,  5.62s/it]
[5/6]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:37,  5.55s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:33,  5.58s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:27,  5.58s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:22,  5.58s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:27<04:16,  5.58s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:11,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:08,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:45<04:07,  5.75s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<04:00,  5.74s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:57<04:00,  5.88s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:03<04:05,  6.14s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:10<04:03,  6.23s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:15<03:49,  6.05s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:22<03:46,  6.12s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:28<03:40,  6.12s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:33<03:29,  5.97s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:39<03:19,  5.87s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:45<03:11,  5.80s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:50<03:03,  5.75s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:56<02:57,  5.72s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [02:02<02:50,  5.69s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:07<02:44,  5.68s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:13<02:38,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:19<02:32,  5.66s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:24<02:26,  5.65s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:30<02:21,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:35<02:15,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:41<02:09,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:47<02:03,  5.64s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:52<01:58,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:58<01:52,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [03:04<01:46,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:09<01:41,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:15<01:35,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:20<01:29,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:26<01:24,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:32<01:18,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:37<01:12,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:43<01:07,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:48<01:01,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:54<00:56,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [04:00<00:50,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:05<00:44,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:11<00:39,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:17<00:33,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:22<00:28,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:28<00:22,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:33<00:16,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:39<00:11,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:45<00:05,  5.62s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:45<00:05,  5.70s/it]
[6/6]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:38,  5.57s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:35,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:29,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:23,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:28<04:18,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:12,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:07,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:01,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:56,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:56<03:50,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:44,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:39,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:13<03:33,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:28,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:24<03:22,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:29<03:16,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:11,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:41<03:05,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<03:00,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:52<02:54,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:58<02:48,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:42,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:09<02:36,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:14<02:31,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:25,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:26<02:19,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:31<02:14,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:37<02:08,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:42<02:03,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:48<01:57,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:54<01:52,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [02:59<01:46,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:40,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  67%|██████▋   | 34/51 [03:10<01:35,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  69%|██████▊   | 35/51 [03:16<01:29,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  71%|███████   | 36/51 [03:22<01:24,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  73%|███████▎  | 37/51 [03:27<01:18,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  75%|███████▍  | 38/51 [03:33<01:13,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  76%|███████▋  | 39/51 [03:39<01:07,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  78%|███████▊  | 40/51 [03:44<01:01,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  80%|████████  | 41/51 [03:50<00:56,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  82%|████████▏ | 42/51 [03:55<00:50,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  84%|████████▍ | 43/51 [04:01<00:45,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  86%|████████▋ | 44/51 [04:07<00:39,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  88%|████████▊ | 45/51 [04:12<00:33,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  90%|█████████ | 46/51 [04:18<00:28,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  92%|█████████▏| 47/51 [04:24<00:22,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  94%|█████████▍| 48/51 [04:29<00:16,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  96%|█████████▌| 49/51 [04:35<00:11,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:40<00:05,  5.62s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  98%|█████████▊| 50/51 [04:40<00:05,  5.62s/it]
                        1it [58:40, 1769.30s/it]                        1it [58:40, 1769.30s/it]Moviepy - Building video ./output_trained/0001-0001-seed_14505/000000_with_audio.mp4.
MoviePy - Writing audio in 000000_with_audioTEMP_MPY_wvf_snd.mp3

chunk:   0%|          | 0/216 [00:00<?, ?it/s, now=None][A
chunk:  60%|██████    | 130/216 [00:00<00:00, 1291.14it/s, now=None][A
                                                                    [A                        1it [58:40, 1769.30s/it]                        1it [58:40, 1769.30s/it]MoviePy - Done.
Moviepy - Writing video ./output_trained/0001-0001-seed_14505/000000_with_audio.mp4


t:   0%|          | 0/245 [00:00<?, ?it/s, now=None][A
t:  23%|██▎       | 57/245 [00:00<00:00, 385.68it/s, now=None][A
t:  39%|███▉      | 96/245 [00:00<00:00, 306.10it/s, now=None][A
t:  53%|█████▎    | 131/245 [00:00<00:00, 298.26it/s, now=None][A
t:  66%|██████▌   | 161/245 [00:00<00:00, 284.24it/s, now=None][A
t:  79%|███████▉  | 194/245 [00:00<00:00, 293.04it/s, now=None][A
t:  94%|█████████▍| 230/245 [00:00<00:00, 307.41it/s, now=None][A
                                                               [A                        1it [58:41, 1769.30s/it]                        1it [58:41, 1769.30s/it]2it [58:41, 1759.45s/it]2025-02-17 09:01:13,589 - INFO - separator - Starting separation process for audio_file_path: examples/inference/audios/0008.wav
Moviepy - Done !
Moviepy - video ready ./output_trained/0001-0001-seed_14505/000000_with_audio.mp4
saving in:  ./output_trained/0001-0001-seed_14505

  0%|          | 0/3 [00:00<?, ?it/s][A
 33%|███▎      | 1/3 [00:00<00:00,  6.22it/s][A
 67%|██████▋   | 2/3 [00:00<00:00,  7.42it/s][A100%|██████████| 3/3 [00:00<00:00,  7.96it/s]

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|██████▋   | 2/3 [00:00<00:00, 15.39it/s][A100%|██████████| 3/3 [00:00<00:00, 15.86it/s]
2025-02-17 09:01:14,220 - INFO - mdx_separator - Saving Vocals stem to 0008_(Vocals)_Kim_Vocal_2.wav...
2025-02-17 09:01:14,221 - INFO - common_separator - Audio duration is 0.00 hours (10.45 seconds).
2025-02-17 09:01:14,221 - INFO - common_separator - Using pydub for writing.
2025-02-17 09:01:14,665 - INFO - common_separator - Clearing input audio file paths, sources and stems...
2025-02-17 09:01:14,665 - INFO - separator - Separation duration: 00:00:01
I0000 00:00:1739782875.697454   70480 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:77) display != EGL_NO_DISPLAYeglGetDisplay() returned error 0x300c
W0000 00:00:1739782875.697926   70480 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.
W0000 00:00:1739782875.704451   15711 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
W0000 00:00:1739782875.727114   15714 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.
txt [128]
[1/7]
##############################  Sampling setting  ##############################
Sampler: VPSDEDPMPP2MSampler
Discretization: ZeroSNRDDPMDiscretization

Sampling with VPSDEDPMPP2MSampler for 51 steps:   0%|          | 0/51 [00:00<?, ?it/s][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   2%|▏         | 1/51 [00:05<04:38,  5.57s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   4%|▍         | 2/51 [00:11<04:34,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   6%|▌         | 3/51 [00:16<04:28,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:   8%|▊         | 4/51 [00:22<04:23,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  10%|▉         | 5/51 [00:27<04:17,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  12%|█▏        | 6/51 [00:33<04:11,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  14%|█▎        | 7/51 [00:39<04:05,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  16%|█▌        | 8/51 [00:44<04:00,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  18%|█▊        | 9/51 [00:50<03:54,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  20%|█▉        | 10/51 [00:55<03:49,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  22%|██▏       | 11/51 [01:01<03:43,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  24%|██▎       | 12/51 [01:07<03:38,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  25%|██▌       | 13/51 [01:12<03:32,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  27%|██▋       | 14/51 [01:18<03:26,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  29%|██▉       | 15/51 [01:23<03:21,  5.59s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  31%|███▏      | 16/51 [01:29<03:15,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  33%|███▎      | 17/51 [01:35<03:10,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  35%|███▌      | 18/51 [01:40<03:04,  5.60s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  37%|███▋      | 19/51 [01:46<02:59,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  39%|███▉      | 20/51 [01:51<02:53,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  41%|████      | 21/51 [01:57<02:48,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  43%|████▎     | 22/51 [02:03<02:42,  5.61s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  45%|████▌     | 23/51 [02:08<02:37,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  47%|████▋     | 24/51 [02:14<02:31,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  49%|████▉     | 25/51 [02:20<02:26,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  51%|█████     | 26/51 [02:25<02:20,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  53%|█████▎    | 27/51 [02:31<02:14,  5.62s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  55%|█████▍    | 28/51 [02:36<02:09,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  57%|█████▋    | 29/51 [02:42<02:03,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  59%|█████▉    | 30/51 [02:48<01:58,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  61%|██████    | 31/51 [02:53<01:52,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  63%|██████▎   | 32/51 [02:59<01:46,  5.63s/it][A
Sampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:05<01:41,  5.63s/it][ASampling with VPSDEDPMPP2MSampler for 51 steps:  65%|██████▍   | 33/51 [03:09<01:43,  5.74s/it]
2it [1:01:54, 1857.10s/it]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sample_video.py", line 450, in <module>
[rank0]:     sampling_main(args, model_cls=SATVideoDiffusionEngine)
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sample_video.py", line 377, in sampling_main
[rank0]:     samples_z = sample_func(
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/diffusion_video.py", line 332, in sample
[rank0]:     samples = self.sampler(denoiser, randn, cond, uc=uc, scale=scale, scale_emb=scale_emb,
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sgm/modules/diffusionmodules/sampling.py", line 678, in __call__
[rank0]:     x, old_denoised = self.sampler_step(
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sgm/modules/diffusionmodules/sampling.py", line 631, in sampler_step
[rank0]:     denoised = self.denoise(
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sgm/modules/diffusionmodules/sampling.py", line 522, in denoise
[rank0]:     denoised = denoiser(
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/diffusion_video.py", line 328, in <lambda>
[rank0]:     denoiser = lambda input, sigma, c, **addtional_model_inputs: self.denoiser(
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sgm/modules/diffusionmodules/denoiser.py", line 39, in forward
[rank0]:     return network(input * c_in, c_noise, cond, **additional_model_inputs) * c_out + input * c_skip
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sgm/modules/diffusionmodules/wrappers.py", line 43, in forward
[rank0]:     return self.diffusion_model(
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/dit_video_concat.py", line 883, in forward
[rank0]:     output = super().forward(**kwargs)[0]
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sgm/models/base_model.py", line 144, in forward
[rank0]:     return self.transformer(*args, **kwargs)
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sgm/models/transformer.py", line 907, in forward
[rank0]:     layer_ret = self.hooks['layer_forward'](*args,
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/dit_video_concat.py", line 552, in layer_forward
[rank0]:     attention_output = layer.attention(attention_input, mask, **kwargs)
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sgm/models/transformer.py", line 124, in forward
[rank0]:     return HOOKS_DEFAULT['attention_forward'](self, hidden_states, mask, **kw_args)
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/sgm/models/transformer_defaults.py", line 107, in attention_forward_default
[rank0]:     context_layer = attention_fn(query_layer, key_layer, value_layer, mask, dropout_fn, **kw_args)
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/dit_video_concat.py", line 637, in attention_fn
[rank0]:     return old_impl(
[rank0]:   File "/wangbenyou/huanghj/workspace/hallo3/hallo3/dit_video_concat.py", line 349, in attention_fn
[rank0]:     return attention_fn_default(
[rank0]:   File "/sds_wangby/group_conda_envs/anaconda3/envs/hallo3/lib/python3.10/site-packages/sat/transformer_defaults.py", line 59, in attention_fn_default
[rank0]:     if int(torch.__version__.split('.')[0]) >= 2 and scaling_attention_score and (is_full or is_low_triangle):
[rank0]: KeyboardInterrupt
