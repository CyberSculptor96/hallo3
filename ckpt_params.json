{
  "model.diffusion_model.mixins.pos_embed.pos_embedding": "torch.Size([1, 17776, 3072])",
  "model.diffusion_model.mixins.pos_embed.freqs_sin": "torch.Size([17550, 64])",
  "model.diffusion_model.mixins.pos_embed.freqs_cos": "torch.Size([17550, 64])",
  "model.diffusion_model.mixins.patch_embed.proj.weight": "torch.Size([3072, 32, 2, 2])",
  "model.diffusion_model.mixins.patch_embed.proj.bias": "torch.Size([3072])",
  "model.diffusion_model.mixins.patch_embed.text_proj.weight": "torch.Size([3072, 4096])",
  "model.diffusion_model.mixins.patch_embed.text_proj.bias": "torch.Size([3072])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.0.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.0.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.1.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.1.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.2.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.2.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.3.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.3.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.4.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.4.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.5.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.5.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.6.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.6.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.7.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.7.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.8.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.8.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.9.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.9.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.10.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.10.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.11.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.11.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.12.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.12.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.13.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.13.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.14.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.14.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.15.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.15.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.16.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.16.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.17.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.17.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.18.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.18.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.19.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.19.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.20.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.20.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.21.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.21.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.22.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.22.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.23.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.23.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.24.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.24.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.25.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.25.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.26.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.26.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.27.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.27.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.28.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.28.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.29.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.29.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.30.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.30.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.31.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.31.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.32.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.32.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.33.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.33.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.34.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.34.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.35.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.35.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.36.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.36.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.37.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.37.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.38.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.38.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.39.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.39.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.40.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.40.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.41.1.weight": "torch.Size([36864, 512])",
  "model.diffusion_model.mixins.adaln_layer.adaLN_modulations.41.1.bias": "torch.Size([36864])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.0.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.0.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.1.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.1.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.2.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.2.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.3.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.3.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.4.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.4.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.5.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.5.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.6.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.6.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.7.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.7.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.8.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.8.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.9.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.9.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.10.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.10.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.11.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.11.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.12.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.12.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.13.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.13.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.14.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.14.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.15.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.15.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.16.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.16.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.17.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.17.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.18.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.18.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.19.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.19.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.20.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.20.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.21.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.21.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.22.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.22.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.23.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.23.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.24.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.24.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.25.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.25.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.26.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.26.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.27.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.27.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.28.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.28.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.29.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.29.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.30.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.30.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.31.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.31.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.32.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.32.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.33.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.33.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.34.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.34.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.35.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.35.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.36.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.36.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.37.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.37.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.38.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.38.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.39.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.39.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.40.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.40.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.41.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.query_layernorm_list.41.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.0.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.0.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.1.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.1.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.2.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.2.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.3.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.3.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.4.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.4.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.5.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.5.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.6.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.6.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.7.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.7.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.8.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.8.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.9.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.9.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.10.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.10.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.11.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.11.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.12.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.12.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.13.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.13.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.14.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.14.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.15.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.15.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.16.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.16.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.17.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.17.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.18.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.18.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.19.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.19.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.20.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.20.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.21.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.21.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.22.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.22.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.23.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.23.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.24.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.24.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.25.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.25.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.26.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.26.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.27.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.27.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.28.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.28.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.29.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.29.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.30.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.30.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.31.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.31.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.32.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.32.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.33.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.33.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.34.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.34.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.35.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.35.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.36.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.36.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.37.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.37.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.38.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.38.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.39.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.39.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.40.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.40.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.41.weight": "torch.Size([64])",
  "model.diffusion_model.mixins.adaln_layer.key_layernorm_list.41.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.final_layer.norm_final.weight": "torch.Size([3072])",
  "model.diffusion_model.mixins.final_layer.norm_final.bias": "torch.Size([3072])",
  "model.diffusion_model.mixins.final_layer.linear.weight": "torch.Size([64, 3072])",
  "model.diffusion_model.mixins.final_layer.linear.bias": "torch.Size([64])",
  "model.diffusion_model.mixins.final_layer.adaLN_modulation.1.weight": "torch.Size([6144, 512])",
  "model.diffusion_model.mixins.final_layer.adaLN_modulation.1.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.position_embeddings.weight": "torch.Size([64, 3072])",
  "model.diffusion_model.transformer.layers.0.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.0.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.0.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.0.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.0.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.0.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.0.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.0.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.0.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.0.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.0.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.0.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.0.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.0.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.0.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.0.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.0.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.0.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.0.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.0.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.1.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.1.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.1.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.1.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.1.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.1.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.1.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.1.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.1.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.1.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.1.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.1.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.1.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.1.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.1.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.1.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.1.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.1.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.1.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.1.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.2.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.2.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.2.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.2.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.2.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.2.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.2.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.2.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.2.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.2.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.2.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.2.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.2.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.2.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.2.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.2.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.2.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.2.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.2.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.2.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.3.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.3.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.3.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.3.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.3.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.3.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.3.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.3.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.3.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.3.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.3.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.3.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.3.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.3.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.3.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.3.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.3.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.3.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.3.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.3.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.4.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.4.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.4.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.4.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.4.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.4.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.4.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.4.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.4.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.4.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.4.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.4.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.4.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.4.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.4.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.4.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.4.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.4.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.4.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.4.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.5.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.5.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.5.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.5.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.5.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.5.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.5.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.5.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.5.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.5.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.5.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.5.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.5.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.5.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.5.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.5.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.5.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.5.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.5.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.5.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.6.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.6.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.6.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.6.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.6.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.6.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.6.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.6.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.6.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.6.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.6.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.6.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.6.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.6.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.6.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.6.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.6.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.6.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.6.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.6.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.7.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.7.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.7.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.7.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.7.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.7.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.7.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.7.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.7.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.7.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.7.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.7.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.7.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.7.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.7.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.7.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.7.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.7.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.7.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.7.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.8.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.8.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.8.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.8.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.8.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.8.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.8.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.8.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.8.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.8.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.8.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.8.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.8.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.8.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.8.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.8.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.8.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.8.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.8.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.8.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.9.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.9.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.9.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.9.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.9.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.9.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.9.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.9.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.9.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.9.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.9.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.9.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.9.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.9.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.9.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.9.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.9.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.9.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.9.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.9.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.10.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.10.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.10.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.10.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.10.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.10.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.10.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.10.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.10.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.10.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.10.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.10.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.10.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.10.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.10.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.10.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.10.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.10.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.10.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.10.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.11.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.11.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.11.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.11.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.11.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.11.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.11.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.11.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.11.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.11.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.11.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.11.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.11.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.11.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.11.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.11.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.11.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.11.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.11.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.11.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.12.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.12.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.12.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.12.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.12.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.12.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.12.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.12.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.12.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.12.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.12.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.12.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.12.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.12.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.12.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.12.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.12.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.12.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.12.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.12.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.13.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.13.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.13.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.13.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.13.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.13.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.13.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.13.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.13.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.13.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.13.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.13.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.13.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.13.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.13.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.13.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.13.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.13.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.13.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.13.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.14.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.14.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.14.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.14.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.14.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.14.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.14.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.14.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.14.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.14.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.14.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.14.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.14.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.14.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.14.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.14.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.14.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.14.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.14.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.14.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.15.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.15.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.15.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.15.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.15.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.15.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.15.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.15.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.15.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.15.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.15.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.15.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.15.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.15.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.15.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.15.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.15.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.15.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.15.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.15.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.16.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.16.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.16.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.16.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.16.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.16.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.16.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.16.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.16.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.16.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.16.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.16.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.16.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.16.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.16.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.16.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.16.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.16.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.16.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.16.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.17.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.17.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.17.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.17.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.17.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.17.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.17.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.17.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.17.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.17.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.17.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.17.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.17.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.17.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.17.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.17.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.17.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.17.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.17.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.17.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.18.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.18.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.18.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.18.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.18.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.18.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.18.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.18.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.18.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.18.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.18.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.18.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.18.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.18.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.18.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.18.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.18.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.18.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.18.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.18.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.19.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.19.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.19.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.19.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.19.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.19.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.19.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.19.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.19.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.19.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.19.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.19.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.19.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.19.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.19.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.19.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.19.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.19.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.19.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.19.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.20.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.20.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.20.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.20.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.20.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.20.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.20.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.20.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.20.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.20.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.20.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.20.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.20.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.20.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.20.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.20.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.20.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.20.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.20.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.20.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.21.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.21.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.21.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.21.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.21.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.21.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.21.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.21.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.21.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.21.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.21.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.21.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.21.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.21.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.21.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.21.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.21.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.21.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.21.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.21.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.22.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.22.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.22.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.22.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.22.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.22.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.22.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.22.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.22.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.22.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.22.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.22.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.22.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.22.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.22.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.22.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.22.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.22.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.22.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.22.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.23.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.23.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.23.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.23.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.23.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.23.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.23.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.23.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.23.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.23.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.23.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.23.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.23.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.23.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.23.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.23.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.23.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.23.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.23.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.23.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.24.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.24.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.24.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.24.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.24.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.24.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.24.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.24.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.24.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.24.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.24.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.24.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.24.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.24.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.24.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.24.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.24.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.24.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.24.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.24.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.25.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.25.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.25.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.25.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.25.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.25.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.25.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.25.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.25.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.25.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.25.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.25.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.25.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.25.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.25.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.25.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.25.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.25.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.25.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.25.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.26.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.26.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.26.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.26.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.26.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.26.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.26.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.26.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.26.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.26.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.26.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.26.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.26.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.26.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.26.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.26.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.26.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.26.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.26.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.26.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.27.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.27.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.27.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.27.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.27.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.27.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.27.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.27.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.27.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.27.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.27.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.27.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.27.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.27.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.27.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.27.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.27.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.27.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.27.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.27.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.28.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.28.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.28.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.28.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.28.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.28.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.28.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.28.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.28.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.28.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.28.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.28.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.28.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.28.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.28.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.28.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.28.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.28.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.28.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.28.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.29.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.29.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.29.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.29.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.29.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.29.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.29.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.29.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.29.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.29.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.29.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.29.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.29.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.29.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.29.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.29.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.29.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.29.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.29.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.29.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.30.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.30.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.30.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.30.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.30.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.30.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.30.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.30.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.30.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.30.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.30.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.30.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.30.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.30.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.30.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.30.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.30.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.30.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.30.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.30.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.31.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.31.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.31.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.31.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.31.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.31.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.31.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.31.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.31.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.31.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.31.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.31.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.31.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.31.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.31.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.31.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.31.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.31.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.31.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.31.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.32.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.32.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.32.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.32.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.32.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.32.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.32.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.32.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.32.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.32.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.32.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.32.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.32.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.32.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.32.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.32.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.32.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.32.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.32.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.32.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.33.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.33.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.33.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.33.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.33.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.33.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.33.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.33.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.33.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.33.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.33.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.33.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.33.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.33.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.33.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.33.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.33.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.33.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.33.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.33.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.34.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.34.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.34.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.34.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.34.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.34.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.34.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.34.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.34.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.34.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.34.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.34.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.34.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.34.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.34.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.34.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.34.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.34.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.34.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.34.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.35.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.35.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.35.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.35.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.35.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.35.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.35.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.35.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.35.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.35.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.35.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.35.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.35.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.35.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.35.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.35.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.35.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.35.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.35.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.35.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.36.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.36.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.36.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.36.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.36.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.36.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.36.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.36.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.36.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.36.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.36.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.36.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.36.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.36.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.36.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.36.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.36.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.36.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.36.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.36.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.37.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.37.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.37.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.37.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.37.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.37.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.37.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.37.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.37.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.37.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.37.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.37.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.37.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.37.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.37.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.37.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.37.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.37.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.37.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.37.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.38.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.38.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.38.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.38.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.38.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.38.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.38.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.38.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.38.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.38.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.38.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.38.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.38.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.38.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.38.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.38.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.38.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.38.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.38.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.38.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.39.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.39.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.39.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.39.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.39.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.39.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.39.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.39.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.39.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.39.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.39.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.39.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.39.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.39.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.39.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.39.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.39.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.39.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.39.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.39.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.40.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.40.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.40.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.40.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.40.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.40.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.40.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.40.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.40.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.40.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.40.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.40.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.40.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.40.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.40.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.40.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.40.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.40.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.40.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.40.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.41.input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.41.input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.41.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "model.diffusion_model.transformer.layers.41.attention.query_key_value.bias": "torch.Size([9216])",
  "model.diffusion_model.transformer.layers.41.attention.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.41.attention.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.41.post_attention_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.41.post_attention_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.41.face_input_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.41.face_input_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.41.face_attn.query.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.41.face_attn.query.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.41.face_attn.key_value.weight": "torch.Size([6144, 1024])",
  "model.diffusion_model.transformer.layers.41.face_attn.key_value.bias": "torch.Size([6144])",
  "model.diffusion_model.transformer.layers.41.face_attn.dense.weight": "torch.Size([3072, 3072])",
  "model.diffusion_model.transformer.layers.41.face_attn.dense.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.layers.41.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "model.diffusion_model.transformer.layers.41.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "model.diffusion_model.transformer.layers.41.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "model.diffusion_model.transformer.layers.41.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.final_layernorm.weight": "torch.Size([3072])",
  "model.diffusion_model.transformer.final_layernorm.bias": "torch.Size([3072])",
  "model.diffusion_model.transformer.face_proj.proj.weight": "torch.Size([4096, 512])",
  "model.diffusion_model.transformer.face_proj.proj.bias": "torch.Size([4096])",
  "model.diffusion_model.transformer.face_proj.norm.weight": "torch.Size([1024])",
  "model.diffusion_model.transformer.face_proj.norm.bias": "torch.Size([1024])",
  "model.diffusion_model.time_embed.0.weight": "torch.Size([512, 3072])",
  "model.diffusion_model.time_embed.0.bias": "torch.Size([512])",
  "model.diffusion_model.time_embed.2.weight": "torch.Size([512, 512])",
  "model.diffusion_model.time_embed.2.bias": "torch.Size([512])",
  "conditioner.embedders.0.transformer.shared.weight": "torch.Size([32128, 4096])",
  "conditioner.embedders.0.transformer.encoder.embed_tokens.weight": "torch.Size([32128, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight": "torch.Size([32, 64])",
  "conditioner.embedders.0.transformer.encoder.block.0.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.0.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.0.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.0.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.0.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.1.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.1.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.1.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.1.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.1.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.1.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.1.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.1.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.1.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.2.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.2.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.2.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.2.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.2.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.2.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.2.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.2.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.2.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.3.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.3.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.3.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.3.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.3.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.3.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.3.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.3.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.3.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.4.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.4.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.4.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.4.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.4.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.4.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.4.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.4.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.4.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.5.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.5.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.5.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.5.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.5.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.5.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.5.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.5.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.5.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.6.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.6.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.6.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.6.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.6.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.6.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.6.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.6.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.6.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.7.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.7.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.7.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.7.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.7.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.7.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.7.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.7.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.7.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.8.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.8.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.8.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.8.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.8.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.8.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.8.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.8.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.8.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.9.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.9.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.9.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.9.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.9.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.9.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.9.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.9.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.9.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.10.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.10.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.10.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.10.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.10.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.10.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.10.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.10.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.10.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.11.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.11.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.11.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.11.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.11.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.11.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.11.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.11.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.11.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.12.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.12.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.12.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.12.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.12.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.12.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.12.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.12.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.12.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.13.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.13.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.13.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.13.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.13.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.13.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.13.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.13.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.13.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.14.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.14.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.14.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.14.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.14.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.14.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.14.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.14.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.14.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.15.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.15.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.15.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.15.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.15.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.15.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.15.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.15.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.15.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.16.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.16.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.16.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.16.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.16.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.16.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.16.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.16.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.16.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.17.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.17.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.17.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.17.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.17.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.17.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.17.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.17.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.17.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.18.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.18.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.18.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.18.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.18.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.18.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.18.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.18.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.18.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.19.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.19.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.19.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.19.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.19.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.19.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.19.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.19.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.19.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.20.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.20.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.20.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.20.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.20.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.20.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.20.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.20.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.20.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.21.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.21.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.21.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.21.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.21.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.21.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.21.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.21.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.21.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.22.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.22.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.22.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.22.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.22.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.22.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.22.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.22.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.22.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.23.layer.0.SelfAttention.q.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.23.layer.0.SelfAttention.k.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.23.layer.0.SelfAttention.v.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.23.layer.0.SelfAttention.o.weight": "torch.Size([4096, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.23.layer.0.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.block.23.layer.1.DenseReluDense.wi_0.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.23.layer.1.DenseReluDense.wi_1.weight": "torch.Size([10240, 4096])",
  "conditioner.embedders.0.transformer.encoder.block.23.layer.1.DenseReluDense.wo.weight": "torch.Size([4096, 10240])",
  "conditioner.embedders.0.transformer.encoder.block.23.layer.1.layer_norm.weight": "torch.Size([4096])",
  "conditioner.embedders.0.transformer.encoder.final_layer_norm.weight": "torch.Size([4096])",
  "first_stage_model.encoder.conv_in.conv.weight": "torch.Size([128, 3, 3, 3, 3])",
  "first_stage_model.encoder.conv_in.conv.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.0.norm1.weight": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.0.norm1.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.0.conv1.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.encoder.down.0.block.0.conv1.conv.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.0.norm2.weight": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.0.norm2.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.0.conv2.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.encoder.down.0.block.0.conv2.conv.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.1.norm1.weight": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.1.norm1.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.1.conv1.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.encoder.down.0.block.1.conv1.conv.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.1.norm2.weight": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.1.norm2.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.1.conv2.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.encoder.down.0.block.1.conv2.conv.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.2.norm1.weight": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.2.norm1.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.2.conv1.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.encoder.down.0.block.2.conv1.conv.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.2.norm2.weight": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.2.norm2.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.block.2.conv2.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.encoder.down.0.block.2.conv2.conv.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.0.downsample.conv.weight": "torch.Size([128, 128, 3, 3])",
  "first_stage_model.encoder.down.0.downsample.conv.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.1.block.0.norm1.weight": "torch.Size([128])",
  "first_stage_model.encoder.down.1.block.0.norm1.bias": "torch.Size([128])",
  "first_stage_model.encoder.down.1.block.0.conv1.conv.weight": "torch.Size([256, 128, 3, 3, 3])",
  "first_stage_model.encoder.down.1.block.0.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.0.norm2.weight": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.0.norm2.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.0.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.encoder.down.1.block.0.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.0.nin_shortcut.weight": "torch.Size([256, 128, 1, 1, 1])",
  "first_stage_model.encoder.down.1.block.0.nin_shortcut.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.1.norm1.weight": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.1.norm1.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.1.conv1.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.encoder.down.1.block.1.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.1.norm2.weight": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.1.norm2.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.1.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.encoder.down.1.block.1.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.2.norm1.weight": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.2.norm1.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.2.conv1.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.encoder.down.1.block.2.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.2.norm2.weight": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.2.norm2.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.1.block.2.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.encoder.down.1.block.2.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.1.downsample.conv.weight": "torch.Size([256, 256, 3, 3])",
  "first_stage_model.encoder.down.1.downsample.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.0.norm1.weight": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.0.norm1.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.0.conv1.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.encoder.down.2.block.0.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.0.norm2.weight": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.0.norm2.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.0.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.encoder.down.2.block.0.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.1.norm1.weight": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.1.norm1.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.1.conv1.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.encoder.down.2.block.1.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.1.norm2.weight": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.1.norm2.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.1.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.encoder.down.2.block.1.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.2.norm1.weight": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.2.norm1.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.2.conv1.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.encoder.down.2.block.2.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.2.norm2.weight": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.2.norm2.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.block.2.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.encoder.down.2.block.2.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.2.downsample.conv.weight": "torch.Size([256, 256, 3, 3])",
  "first_stage_model.encoder.down.2.downsample.conv.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.3.block.0.norm1.weight": "torch.Size([256])",
  "first_stage_model.encoder.down.3.block.0.norm1.bias": "torch.Size([256])",
  "first_stage_model.encoder.down.3.block.0.conv1.conv.weight": "torch.Size([512, 256, 3, 3, 3])",
  "first_stage_model.encoder.down.3.block.0.conv1.conv.bias": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.0.norm2.weight": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.0.norm2.bias": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.0.conv2.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.encoder.down.3.block.0.conv2.conv.bias": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.0.nin_shortcut.weight": "torch.Size([512, 256, 1, 1, 1])",
  "first_stage_model.encoder.down.3.block.0.nin_shortcut.bias": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.1.norm1.weight": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.1.norm1.bias": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.1.conv1.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.encoder.down.3.block.1.conv1.conv.bias": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.1.norm2.weight": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.1.norm2.bias": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.1.conv2.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.encoder.down.3.block.1.conv2.conv.bias": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.2.norm1.weight": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.2.norm1.bias": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.2.conv1.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.encoder.down.3.block.2.conv1.conv.bias": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.2.norm2.weight": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.2.norm2.bias": "torch.Size([512])",
  "first_stage_model.encoder.down.3.block.2.conv2.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.encoder.down.3.block.2.conv2.conv.bias": "torch.Size([512])",
  "first_stage_model.encoder.mid.block_1.norm1.weight": "torch.Size([512])",
  "first_stage_model.encoder.mid.block_1.norm1.bias": "torch.Size([512])",
  "first_stage_model.encoder.mid.block_1.conv1.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.encoder.mid.block_1.conv1.conv.bias": "torch.Size([512])",
  "first_stage_model.encoder.mid.block_1.norm2.weight": "torch.Size([512])",
  "first_stage_model.encoder.mid.block_1.norm2.bias": "torch.Size([512])",
  "first_stage_model.encoder.mid.block_1.conv2.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.encoder.mid.block_1.conv2.conv.bias": "torch.Size([512])",
  "first_stage_model.encoder.mid.block_2.norm1.weight": "torch.Size([512])",
  "first_stage_model.encoder.mid.block_2.norm1.bias": "torch.Size([512])",
  "first_stage_model.encoder.mid.block_2.conv1.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.encoder.mid.block_2.conv1.conv.bias": "torch.Size([512])",
  "first_stage_model.encoder.mid.block_2.norm2.weight": "torch.Size([512])",
  "first_stage_model.encoder.mid.block_2.norm2.bias": "torch.Size([512])",
  "first_stage_model.encoder.mid.block_2.conv2.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.encoder.mid.block_2.conv2.conv.bias": "torch.Size([512])",
  "first_stage_model.encoder.norm_out.weight": "torch.Size([512])",
  "first_stage_model.encoder.norm_out.bias": "torch.Size([512])",
  "first_stage_model.encoder.conv_out.conv.weight": "torch.Size([32, 512, 3, 3, 3])",
  "first_stage_model.encoder.conv_out.conv.bias": "torch.Size([32])",
  "first_stage_model.decoder.conv_in.conv.weight": "torch.Size([512, 16, 3, 3, 3])",
  "first_stage_model.decoder.conv_in.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_1.norm1.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_1.norm1.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_1.norm1.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.mid.block_1.norm1.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_1.norm1.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.mid.block_1.norm1.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_1.conv1.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.decoder.mid.block_1.conv1.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_1.norm2.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_1.norm2.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_1.norm2.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.mid.block_1.norm2.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_1.norm2.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.mid.block_1.norm2.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_1.conv2.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.decoder.mid.block_1.conv2.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_2.norm1.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_2.norm1.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_2.norm1.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.mid.block_2.norm1.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_2.norm1.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.mid.block_2.norm1.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_2.conv1.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.decoder.mid.block_2.conv1.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_2.norm2.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_2.norm2.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_2.norm2.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.mid.block_2.norm2.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_2.norm2.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.mid.block_2.norm2.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.mid.block_2.conv2.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.decoder.mid.block_2.conv2.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.0.block.0.norm1.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.0.block.0.norm1.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.0.block.0.norm1.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.0.norm1.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.0.block.0.norm1.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.0.norm1.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.0.block.0.conv1.conv.weight": "torch.Size([128, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.0.block.0.conv1.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.0.norm2.norm_layer.weight": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.0.norm2.norm_layer.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.0.norm2.conv_y.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.0.norm2.conv_y.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.0.norm2.conv_b.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.0.norm2.conv_b.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.0.conv2.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.decoder.up.0.block.0.conv2.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.0.nin_shortcut.weight": "torch.Size([128, 256, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.0.nin_shortcut.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.1.norm1.norm_layer.weight": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.1.norm1.norm_layer.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.1.norm1.conv_y.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.1.norm1.conv_y.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.1.norm1.conv_b.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.1.norm1.conv_b.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.1.conv1.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.decoder.up.0.block.1.conv1.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.1.norm2.norm_layer.weight": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.1.norm2.norm_layer.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.1.norm2.conv_y.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.1.norm2.conv_y.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.1.norm2.conv_b.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.1.norm2.conv_b.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.1.conv2.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.decoder.up.0.block.1.conv2.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.2.norm1.norm_layer.weight": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.2.norm1.norm_layer.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.2.norm1.conv_y.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.2.norm1.conv_y.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.2.norm1.conv_b.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.2.norm1.conv_b.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.2.conv1.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.decoder.up.0.block.2.conv1.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.2.norm2.norm_layer.weight": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.2.norm2.norm_layer.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.2.norm2.conv_y.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.2.norm2.conv_y.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.2.norm2.conv_b.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.2.norm2.conv_b.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.2.conv2.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.decoder.up.0.block.2.conv2.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.3.norm1.norm_layer.weight": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.3.norm1.norm_layer.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.3.norm1.conv_y.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.3.norm1.conv_y.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.3.norm1.conv_b.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.3.norm1.conv_b.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.3.conv1.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.decoder.up.0.block.3.conv1.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.3.norm2.norm_layer.weight": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.3.norm2.norm_layer.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.3.norm2.conv_y.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.3.norm2.conv_y.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.3.norm2.conv_b.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.0.block.3.norm2.conv_b.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.0.block.3.conv2.conv.weight": "torch.Size([128, 128, 3, 3, 3])",
  "first_stage_model.decoder.up.0.block.3.conv2.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.up.1.block.0.norm1.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.0.norm1.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.0.norm1.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.0.norm1.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.0.norm1.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.0.norm1.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.0.conv1.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.1.block.0.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.0.norm2.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.0.norm2.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.0.norm2.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.0.norm2.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.0.norm2.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.0.norm2.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.0.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.1.block.0.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.1.norm1.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.1.norm1.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.1.norm1.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.1.norm1.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.1.norm1.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.1.norm1.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.1.conv1.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.1.block.1.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.1.norm2.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.1.norm2.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.1.norm2.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.1.norm2.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.1.norm2.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.1.norm2.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.1.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.1.block.1.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.2.norm1.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.2.norm1.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.2.norm1.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.2.norm1.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.2.norm1.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.2.norm1.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.2.conv1.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.1.block.2.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.2.norm2.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.2.norm2.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.2.norm2.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.2.norm2.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.2.norm2.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.2.norm2.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.2.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.1.block.2.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.3.norm1.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.3.norm1.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.3.norm1.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.3.norm1.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.3.norm1.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.3.norm1.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.3.conv1.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.1.block.3.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.3.norm2.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.3.norm2.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.3.norm2.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.3.norm2.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.3.norm2.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.1.block.3.norm2.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.block.3.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.1.block.3.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.1.upsample.conv.weight": "torch.Size([256, 256, 3, 3])",
  "first_stage_model.decoder.up.1.upsample.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.0.norm1.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.up.2.block.0.norm1.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.2.block.0.norm1.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.0.norm1.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.2.block.0.norm1.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.0.norm1.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.2.block.0.conv1.conv.weight": "torch.Size([256, 512, 3, 3, 3])",
  "first_stage_model.decoder.up.2.block.0.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.0.norm2.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.0.norm2.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.0.norm2.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.0.norm2.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.0.norm2.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.0.norm2.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.0.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.2.block.0.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.0.nin_shortcut.weight": "torch.Size([256, 512, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.0.nin_shortcut.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.1.norm1.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.1.norm1.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.1.norm1.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.1.norm1.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.1.norm1.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.1.norm1.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.1.conv1.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.2.block.1.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.1.norm2.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.1.norm2.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.1.norm2.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.1.norm2.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.1.norm2.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.1.norm2.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.1.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.2.block.1.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.2.norm1.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.2.norm1.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.2.norm1.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.2.norm1.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.2.norm1.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.2.norm1.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.2.conv1.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.2.block.2.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.2.norm2.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.2.norm2.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.2.norm2.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.2.norm2.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.2.norm2.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.2.norm2.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.2.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.2.block.2.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.3.norm1.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.3.norm1.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.3.norm1.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.3.norm1.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.3.norm1.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.3.norm1.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.3.conv1.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.2.block.3.conv1.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.3.norm2.norm_layer.weight": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.3.norm2.norm_layer.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.3.norm2.conv_y.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.3.norm2.conv_y.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.3.norm2.conv_b.conv.weight": "torch.Size([256, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.2.block.3.norm2.conv_b.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.block.3.conv2.conv.weight": "torch.Size([256, 256, 3, 3, 3])",
  "first_stage_model.decoder.up.2.block.3.conv2.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.2.upsample.conv.weight": "torch.Size([256, 256, 3, 3])",
  "first_stage_model.decoder.up.2.upsample.conv.bias": "torch.Size([256])",
  "first_stage_model.decoder.up.3.block.0.norm1.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.0.norm1.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.0.norm1.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.0.norm1.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.0.norm1.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.0.norm1.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.0.conv1.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.decoder.up.3.block.0.conv1.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.0.norm2.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.0.norm2.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.0.norm2.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.0.norm2.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.0.norm2.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.0.norm2.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.0.conv2.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.decoder.up.3.block.0.conv2.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.1.norm1.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.1.norm1.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.1.norm1.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.1.norm1.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.1.norm1.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.1.norm1.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.1.conv1.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.decoder.up.3.block.1.conv1.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.1.norm2.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.1.norm2.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.1.norm2.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.1.norm2.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.1.norm2.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.1.norm2.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.1.conv2.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.decoder.up.3.block.1.conv2.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.2.norm1.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.2.norm1.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.2.norm1.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.2.norm1.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.2.norm1.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.2.norm1.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.2.conv1.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.decoder.up.3.block.2.conv1.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.2.norm2.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.2.norm2.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.2.norm2.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.2.norm2.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.2.norm2.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.2.norm2.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.2.conv2.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.decoder.up.3.block.2.conv2.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.3.norm1.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.3.norm1.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.3.norm1.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.3.norm1.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.3.norm1.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.3.norm1.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.3.conv1.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.decoder.up.3.block.3.conv1.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.3.norm2.norm_layer.weight": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.3.norm2.norm_layer.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.3.norm2.conv_y.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.3.norm2.conv_y.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.3.norm2.conv_b.conv.weight": "torch.Size([512, 16, 1, 1, 1])",
  "first_stage_model.decoder.up.3.block.3.norm2.conv_b.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.block.3.conv2.conv.weight": "torch.Size([512, 512, 3, 3, 3])",
  "first_stage_model.decoder.up.3.block.3.conv2.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.up.3.upsample.conv.weight": "torch.Size([512, 512, 3, 3])",
  "first_stage_model.decoder.up.3.upsample.conv.bias": "torch.Size([512])",
  "first_stage_model.decoder.norm_out.norm_layer.weight": "torch.Size([128])",
  "first_stage_model.decoder.norm_out.norm_layer.bias": "torch.Size([128])",
  "first_stage_model.decoder.norm_out.conv_y.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.norm_out.conv_y.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.norm_out.conv_b.conv.weight": "torch.Size([128, 16, 1, 1, 1])",
  "first_stage_model.decoder.norm_out.conv_b.conv.bias": "torch.Size([128])",
  "first_stage_model.decoder.conv_out.conv.weight": "torch.Size([3, 128, 3, 3, 3])",
  "first_stage_model.decoder.conv_out.conv.bias": "torch.Size([3])",
  "ref_model.diffusion_model.mixins.pos_embed.pos_embedding": "torch.Size([1, 17776, 3072])",
  "ref_model.diffusion_model.mixins.pos_embed.freqs_sin": "torch.Size([17550, 64])",
  "ref_model.diffusion_model.mixins.pos_embed.freqs_cos": "torch.Size([17550, 64])",
  "ref_model.diffusion_model.mixins.patch_embed.proj.weight": "torch.Size([3072, 32, 2, 2])",
  "ref_model.diffusion_model.mixins.patch_embed.proj.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.mixins.patch_embed.text_proj.weight": "torch.Size([3072, 4096])",
  "ref_model.diffusion_model.mixins.patch_embed.text_proj.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.0.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.0.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.1.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.1.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.2.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.2.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.3.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.3.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.4.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.4.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.5.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.5.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.6.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.6.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.7.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.7.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.8.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.8.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.9.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.9.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.10.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.10.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.11.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.11.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.12.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.12.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.13.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.13.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.14.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.14.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.15.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.15.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.16.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.16.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.17.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.17.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.18.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.18.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.19.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.19.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.20.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.20.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.21.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.21.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.22.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.22.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.23.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.23.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.24.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.24.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.25.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.25.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.26.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.26.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.27.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.27.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.28.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.28.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.29.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.29.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.30.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.30.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.31.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.31.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.32.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.32.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.33.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.33.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.34.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.34.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.35.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.35.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.36.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.36.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.37.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.37.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.38.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.38.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.39.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.39.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.40.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.40.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.41.1.weight": "torch.Size([36864, 512])",
  "ref_model.diffusion_model.mixins.adaln_layer.adaLN_modulations.41.1.bias": "torch.Size([36864])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.0.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.0.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.1.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.1.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.2.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.2.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.3.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.3.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.4.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.4.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.5.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.5.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.6.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.6.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.7.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.7.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.8.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.8.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.9.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.9.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.10.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.10.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.11.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.11.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.12.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.12.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.13.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.13.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.14.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.14.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.15.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.15.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.16.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.16.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.17.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.17.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.18.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.18.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.19.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.19.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.20.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.20.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.21.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.21.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.22.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.22.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.23.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.23.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.24.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.24.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.25.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.25.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.26.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.26.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.27.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.27.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.28.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.28.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.29.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.29.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.30.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.30.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.31.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.31.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.32.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.32.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.33.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.33.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.34.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.34.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.35.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.35.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.36.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.36.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.37.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.37.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.38.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.38.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.39.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.39.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.40.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.40.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.41.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.query_layernorm_list.41.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.0.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.0.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.1.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.1.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.2.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.2.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.3.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.3.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.4.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.4.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.5.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.5.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.6.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.6.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.7.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.7.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.8.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.8.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.9.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.9.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.10.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.10.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.11.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.11.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.12.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.12.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.13.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.13.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.14.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.14.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.15.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.15.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.16.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.16.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.17.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.17.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.18.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.18.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.19.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.19.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.20.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.20.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.21.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.21.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.22.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.22.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.23.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.23.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.24.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.24.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.25.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.25.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.26.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.26.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.27.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.27.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.28.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.28.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.29.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.29.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.30.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.30.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.31.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.31.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.32.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.32.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.33.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.33.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.34.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.34.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.35.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.35.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.36.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.36.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.37.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.37.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.38.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.38.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.39.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.39.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.40.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.40.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.41.weight": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.adaln_layer.key_layernorm_list.41.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.final_layer.norm_final.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.mixins.final_layer.norm_final.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.mixins.final_layer.linear.weight": "torch.Size([64, 3072])",
  "ref_model.diffusion_model.mixins.final_layer.linear.bias": "torch.Size([64])",
  "ref_model.diffusion_model.mixins.final_layer.adaLN_modulation.1.weight": "torch.Size([6144, 512])",
  "ref_model.diffusion_model.mixins.final_layer.adaLN_modulation.1.bias": "torch.Size([6144])",
  "ref_model.diffusion_model.transformer.position_embeddings.weight": "torch.Size([64, 3072])",
  "ref_model.diffusion_model.transformer.layers.0.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.0.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.0.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.0.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.0.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.0.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.0.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.0.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.0.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.0.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.0.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.0.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.1.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.1.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.1.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.1.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.1.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.1.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.1.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.1.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.1.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.1.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.1.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.1.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.2.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.2.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.2.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.2.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.2.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.2.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.2.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.2.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.2.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.2.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.2.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.2.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.3.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.3.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.3.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.3.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.3.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.3.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.3.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.3.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.3.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.3.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.3.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.3.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.4.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.4.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.4.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.4.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.4.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.4.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.4.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.4.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.4.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.4.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.4.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.4.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.5.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.5.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.5.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.5.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.5.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.5.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.5.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.5.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.5.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.5.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.5.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.5.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.6.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.6.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.6.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.6.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.6.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.6.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.6.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.6.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.6.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.6.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.6.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.6.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.7.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.7.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.7.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.7.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.7.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.7.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.7.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.7.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.7.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.7.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.7.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.7.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.8.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.8.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.8.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.8.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.8.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.8.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.8.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.8.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.8.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.8.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.8.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.8.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.9.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.9.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.9.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.9.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.9.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.9.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.9.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.9.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.9.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.9.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.9.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.9.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.10.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.10.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.10.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.10.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.10.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.10.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.10.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.10.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.10.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.10.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.10.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.10.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.11.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.11.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.11.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.11.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.11.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.11.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.11.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.11.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.11.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.11.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.11.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.11.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.12.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.12.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.12.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.12.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.12.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.12.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.12.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.12.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.12.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.12.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.12.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.12.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.13.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.13.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.13.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.13.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.13.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.13.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.13.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.13.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.13.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.13.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.13.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.13.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.14.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.14.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.14.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.14.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.14.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.14.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.14.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.14.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.14.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.14.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.14.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.14.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.15.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.15.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.15.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.15.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.15.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.15.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.15.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.15.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.15.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.15.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.15.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.15.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.16.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.16.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.16.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.16.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.16.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.16.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.16.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.16.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.16.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.16.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.16.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.16.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.17.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.17.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.17.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.17.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.17.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.17.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.17.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.17.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.17.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.17.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.17.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.17.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.18.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.18.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.18.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.18.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.18.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.18.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.18.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.18.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.18.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.18.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.18.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.18.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.19.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.19.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.19.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.19.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.19.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.19.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.19.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.19.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.19.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.19.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.19.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.19.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.20.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.20.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.20.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.20.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.20.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.20.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.20.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.20.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.20.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.20.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.20.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.20.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.21.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.21.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.21.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.21.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.21.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.21.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.21.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.21.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.21.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.21.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.21.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.21.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.22.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.22.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.22.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.22.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.22.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.22.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.22.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.22.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.22.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.22.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.22.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.22.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.23.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.23.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.23.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.23.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.23.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.23.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.23.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.23.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.23.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.23.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.23.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.23.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.24.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.24.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.24.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.24.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.24.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.24.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.24.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.24.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.24.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.24.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.24.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.24.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.25.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.25.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.25.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.25.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.25.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.25.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.25.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.25.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.25.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.25.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.25.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.25.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.26.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.26.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.26.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.26.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.26.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.26.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.26.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.26.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.26.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.26.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.26.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.26.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.27.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.27.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.27.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.27.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.27.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.27.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.27.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.27.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.27.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.27.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.27.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.27.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.28.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.28.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.28.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.28.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.28.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.28.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.28.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.28.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.28.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.28.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.28.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.28.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.29.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.29.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.29.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.29.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.29.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.29.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.29.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.29.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.29.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.29.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.29.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.29.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.30.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.30.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.30.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.30.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.30.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.30.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.30.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.30.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.30.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.30.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.30.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.30.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.31.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.31.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.31.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.31.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.31.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.31.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.31.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.31.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.31.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.31.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.31.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.31.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.32.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.32.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.32.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.32.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.32.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.32.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.32.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.32.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.32.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.32.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.32.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.32.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.33.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.33.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.33.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.33.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.33.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.33.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.33.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.33.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.33.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.33.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.33.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.33.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.34.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.34.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.34.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.34.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.34.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.34.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.34.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.34.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.34.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.34.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.34.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.34.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.35.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.35.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.35.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.35.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.35.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.35.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.35.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.35.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.35.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.35.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.35.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.35.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.36.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.36.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.36.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.36.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.36.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.36.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.36.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.36.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.36.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.36.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.36.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.36.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.37.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.37.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.37.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.37.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.37.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.37.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.37.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.37.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.37.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.37.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.37.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.37.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.38.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.38.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.38.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.38.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.38.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.38.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.38.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.38.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.38.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.38.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.38.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.38.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.39.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.39.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.39.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.39.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.39.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.39.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.39.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.39.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.39.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.39.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.39.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.39.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.40.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.40.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.40.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.40.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.40.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.40.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.40.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.40.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.40.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.40.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.40.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.40.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.41.input_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.41.input_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.41.attention.query_key_value.weight": "torch.Size([9216, 3072])",
  "ref_model.diffusion_model.transformer.layers.41.attention.query_key_value.bias": "torch.Size([9216])",
  "ref_model.diffusion_model.transformer.layers.41.attention.dense.weight": "torch.Size([3072, 3072])",
  "ref_model.diffusion_model.transformer.layers.41.attention.dense.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.41.post_attention_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.41.post_attention_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.layers.41.mlp.dense_h_to_4h.weight": "torch.Size([12288, 3072])",
  "ref_model.diffusion_model.transformer.layers.41.mlp.dense_h_to_4h.bias": "torch.Size([12288])",
  "ref_model.diffusion_model.transformer.layers.41.mlp.dense_4h_to_h.weight": "torch.Size([3072, 12288])",
  "ref_model.diffusion_model.transformer.layers.41.mlp.dense_4h_to_h.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.final_layernorm.weight": "torch.Size([3072])",
  "ref_model.diffusion_model.transformer.final_layernorm.bias": "torch.Size([3072])",
  "ref_model.diffusion_model.time_embed.0.weight": "torch.Size([512, 3072])",
  "ref_model.diffusion_model.time_embed.0.bias": "torch.Size([512])",
  "ref_model.diffusion_model.time_embed.2.weight": "torch.Size([512, 512])",
  "ref_model.diffusion_model.time_embed.2.bias": "torch.Size([512])"
}